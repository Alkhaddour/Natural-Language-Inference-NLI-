{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electrical-astrology",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "In the previous <a href=\"https://github.com/Alkhaddour/Natural-Language-Inference-NLI-/blob/main/Training%20RoBERTa-based%20model%20using%20STS.ipynb\">notebook</a>, we trained a RoBERTa-based model using STS benchmark. In this notebook, we will fine-tune the same model using <a href=\"https://gluebenchmark.com/\">GLUE</a> benchmark (mainly <a href=\"https://www.microsoft.com/en-us/download/details.aspx?id=52398\">MRPC</a> dataset).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-twins",
   "metadata": {},
   "source": [
    "#### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "relevant-cement",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import RandomSampler, DataLoader, SequentialSampler, TensorDataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "from colorama import Fore\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-humidity",
   "metadata": {},
   "source": [
    "#### Set accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "multiple-lucas",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-cardiff",
   "metadata": {},
   "source": [
    "#### Set data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "outside-juvenile",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = './models'\n",
    "pretrained_STS= 'pretrained_STS.pkl'\n",
    "fineTunedModel = 'model.pkl'\n",
    "FROM_FILE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "closed-finland",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-newsletter",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "twenty-standing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16           \n",
    "max_seq_length = 128\n",
    "lr1 = 1e-3             # learning rate while training the additional layers\n",
    "lr2 = 1e-6             # learning rate while training the whole model\n",
    "NUM_EPOCHS_1 = 25      # Number of epochs used to train new layers\n",
    "NUM_EPOCHS_2 = 50      # Number of epochs used to train the whole new layers\n",
    "dropout_rate = 0.30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-infection",
   "metadata": {},
   "source": [
    "#### Set tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exact-county",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-kinase",
   "metadata": {},
   "source": [
    "#### Define dataset reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "promotional-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def truncate_pair_of_tokens(self, tokens_a, tokens_b):\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= self.max_len - 3:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "     \n",
    "    def build_features(self, example):\n",
    "        tokens_a = tokenizer.tokenize(example[\"sentence1\"])\n",
    "        tokens_b = tokenizer.tokenize(example[\"sentence2\"])\n",
    "        self.truncate_pair_of_tokens(tokens_a, tokens_b)\n",
    "        tokens = []\n",
    "        #tokens.append(\"[CLS]\")\n",
    "        tokens.append(tokenizer.cls_token)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "        #tokens.append(\"[SEP]\")\n",
    "        tokens.append(tokenizer.sep_token) \n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "        #tokens.append(\"[SEP]\")\n",
    "        tokens.append(tokenizer.sep_token)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        while len(input_ids) < self.max_len:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "\n",
    "        input_ids   = torch.tensor(input_ids, dtype=torch.int64).to(device)\n",
    "        input_mask  = torch.tensor(input_mask, dtype=torch.float).to(device)\n",
    "        label       = torch.tensor(example[\"label\"], dtype=torch.int64).to(device)\n",
    "        return (input_ids, input_mask, label)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crucial-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class glueDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, tokenizer):\n",
    "        self.max_len = max_len \n",
    "        self.dataset = dataset\n",
    "        self.feature_extractor =  FeatureExtractor(tokenizer, max_len)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        input_ids, input_mask, label = self.feature_extractor.build_features(example)\n",
    "        return (input_ids, input_mask, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-preview",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define datasets & Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accepted-birthday",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\alkha\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\alkha\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\alkha\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "raw_train_data = load_dataset('glue', 'mrpc', split='train')\n",
    "raw_val_data   = load_dataset('glue', 'mrpc', split='validation')\n",
    "raw_test_data  = load_dataset('glue', 'mrpc', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "printable-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = glueDataset(raw_train_data, max_seq_length,tokenizer)\n",
    "val_dataset   = glueDataset(raw_val_data,   max_seq_length,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "solved-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, shuffle=True,  batch_size=batch_size)\n",
    "val_data_loader   = DataLoader(val_dataset,   shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-electricity",
   "metadata": {},
   "source": [
    "#### Results manager\n",
    "Results manager is used to load and save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "split-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsManager():\n",
    "    def __init__(self, train_len, val_len, output_dir):\n",
    "        self.train_losses  = []\n",
    "        self.val_losses    = []\n",
    "        self.steps         = []\n",
    "        self.best_val_loss = float('Inf')\n",
    "        self.train_len     = train_len\n",
    "        self.val_len       = val_len\n",
    "        self.output_dir    = output_dir\n",
    "          \n",
    "    def save_checkpoint(self, filename, model, valid_loss):\n",
    "        torch.save({'model_state_dict': model.state_dict(),'valid_loss': valid_loss}, os.path.join(self.output_dir, filename))\n",
    "\n",
    "    def load_checkpoint(self, filename, model):    \n",
    "        state_dict = torch.load(os.path.join(self.output_dir , filename), map_location=device)\n",
    "        model.load_state_dict(state_dict['model_state_dict'])\n",
    "        return state_dict['valid_loss']\n",
    "\n",
    "    def save_metrics(self, filename):   \n",
    "        state_dict = {'train_losses': self.train_losses,\n",
    "                      'val_losses': self.val_losses,\n",
    "                      'steps': self.steps}\n",
    "\n",
    "        torch.save(state_dict, os.path.join(self.output_dir, filename))\n",
    "  \n",
    "    def load_metrics(self, filename):    \n",
    "        state_dict = torch.load(os.path.join(self.output_dir , filename), map_location=device)\n",
    "        return state_dict['train_losses'], state_dict['val_losses'], state_dict['steps']\n",
    "\n",
    "    def update_train_val_loss(self, model, train_loss, val_loss, step, epoch, num_epochs):\n",
    "        train_loss = train_loss \n",
    "        val_loss   = val_loss  \n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.steps.append(step)\n",
    "        print('Epoch [{}/{}], step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "              .format(epoch+1, num_epochs, step, num_epochs * self.train_len, train_loss, val_loss))\n",
    "    \n",
    "        # checkpoint\n",
    "        if self.best_val_loss > val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.save_checkpoint('FineTuned_model.pkl', model, self.best_val_loss)\n",
    "            self.save_metrics('FineTuned_metric.pkl')\n",
    "            \n",
    "results = ResultsManager(len(train_dataset), len(val_dataset), output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-ordinary",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-purpose",
   "metadata": {},
   "source": [
    "Before we can fine-tune the model on MRPC dataset, we need to make a change in the last layer, because STS benchmark designates a regression problem (i.e. the input is two sentences, and the output is the similarity between them) while MRPC designates a classification probelm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-wilson",
   "metadata": {},
   "source": [
    "From predefined model, we will take all layers, except the last one, which will be replaced with one for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "complete-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROBERTAClassifier(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(ROBERTAClassifier, self).__init__()        \n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.d1      = torch.nn.Dropout(dropout_rate)\n",
    "        self.l1      = torch.nn.Linear(768, 128)\n",
    "        self.bn1     = torch.nn.LayerNorm(128)\n",
    "        self.d2      = torch.nn.Dropout(dropout_rate)\n",
    "        self.l2      = torch.nn.Linear(128, 1)                # This layer will be ignored in forward\n",
    "        torch.nn.init.xavier_uniform_(self.l1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.l2.weight)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x    = self.d1(x)\n",
    "        x    = self.l1(x)\n",
    "        x    = self.bn1(x)\n",
    "        x    = torch.nn.Tanh()(x)\n",
    "        x    = self.d2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-hypothesis",
   "metadata": {},
   "source": [
    "##### Fine-Tuning: From regression to classification\n",
    "Just add the classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "supreme-preliminary",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ROBERTAClassifier_2(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(ROBERTAClassifier_2, self).__init__()   \n",
    "        self.part_model = ROBERTAClassifier(dropout_rate)\n",
    "        results.load_checkpoint(pretrained_STS, self.part_model)\n",
    "        self.l2 = torch.nn.Linear(128, 2)\n",
    "        self.l3 = torch.nn.Softmax(1)\n",
    "        torch.nn.init.xavier_uniform_(self.l2.weight)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x    = self.part_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x    = self.l2(x)  \n",
    "        if self.training == False:\n",
    "            x  = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-summer",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Main train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "billion-biography",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on cuda\n"
     ]
    }
   ],
   "source": [
    "model = ROBERTAClassifier_2(dropout_rate)\n",
    "model.to(device)\n",
    "print(\"Model on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "attached-tongue",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_data_loader, val_data_loader, results, \n",
    "          scheduler = None, num_epochs = 5 , train_whole_model = False):\n",
    "    step = 0\n",
    "    # if we want to train all the model (our added layers + RoBERTa)\n",
    "    if train_whole_model:\n",
    "        for param in model.part_model.parameters():\n",
    "            param.requires_grad = True\n",
    "    # in case we just want to train our added layer.\n",
    "    else:\n",
    "        for param in model.part_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss  = 0.0                \n",
    "        val_loss    = 0.0\n",
    "        batch_count = 0\n",
    "        for (input_ids, input_mask, y_true) in train_data_loader:\n",
    "            y_pred = model(input_ids = input_ids, attention_mask = input_mask) \n",
    "            loss = torch.nn.CrossEntropyLoss()(y_pred, y_true)\n",
    "            loss.backward()\n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            # Update train loss and step\n",
    "            train_loss += loss.item()\n",
    "            step += batch_size\n",
    "            batch_count+=1\n",
    "        train_loss /= batch_count\n",
    "        model.eval()\n",
    "        with torch.no_grad():      \n",
    "            batch_count = 0\n",
    "            for (input_ids, input_mask, y_true) in val_data_loader:\n",
    "                y_pred = model(input_ids = input_ids, attention_mask = input_mask) \n",
    "                loss = torch.nn.CrossEntropyLoss()(y_pred, y_true)\n",
    "                val_loss += loss.item()\n",
    "                batch_count+=1\n",
    "            val_loss /= batch_count\n",
    "        results.update_train_val_loss(model, train_loss, val_loss, step, epoch, num_epochs)       \n",
    "        model.train()\n",
    "\n",
    "    results.save_metrics('FineTuned_metric.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-barrel",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create or load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "reliable-timothy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = ResultsManager(len(train_dataset), len(val_dataset), output_dir)\n",
    "steps_per_epoch = len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "concerned-scope",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FROM_FILE:\n",
    "    model = ROBERTAClassifier_2(dropout_rate)\n",
    "    results.load_checkpoint(fineTunedModel, model)\n",
    "else:    \n",
    "    model = ROBERTAClassifier_2(dropout_rate)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-approval",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train new layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-switch",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Define optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accomplished-registration",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,  num_warmup_steps=steps_per_epoch * 1, num_training_steps = steps_per_epoch * NUM_EPOCHS_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "spanish-coast",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-11 14:14:53.812779] -- Training new layers started\n",
      "Epoch [1/25], step [3680/91700], Train Loss: 0.6520, Valid Loss: 0.6463\n",
      "Epoch [2/25], step [7360/91700], Train Loss: 0.6415, Valid Loss: 0.6382\n",
      "Epoch [3/25], step [11040/91700], Train Loss: 0.6333, Valid Loss: 0.6332\n",
      "Epoch [4/25], step [14720/91700], Train Loss: 0.6196, Valid Loss: 0.6273\n",
      "Epoch [5/25], step [18400/91700], Train Loss: 0.6047, Valid Loss: 0.6197\n",
      "Epoch [6/25], step [22080/91700], Train Loss: 0.5955, Valid Loss: 0.6126\n",
      "Epoch [7/25], step [25760/91700], Train Loss: 0.5792, Valid Loss: 0.6072\n",
      "Epoch [8/25], step [29440/91700], Train Loss: 0.5601, Valid Loss: 0.5975\n",
      "Epoch [9/25], step [33120/91700], Train Loss: 0.5531, Valid Loss: 0.5895\n",
      "Epoch [10/25], step [36800/91700], Train Loss: 0.5421, Valid Loss: 0.5813\n",
      "Epoch [11/25], step [40480/91700], Train Loss: 0.5337, Valid Loss: 0.5788\n",
      "Epoch [12/25], step [44160/91700], Train Loss: 0.5246, Valid Loss: 0.5704\n",
      "Epoch [13/25], step [47840/91700], Train Loss: 0.5171, Valid Loss: 0.5654\n",
      "Epoch [14/25], step [51520/91700], Train Loss: 0.5192, Valid Loss: 0.5637\n",
      "Epoch [15/25], step [55200/91700], Train Loss: 0.5093, Valid Loss: 0.5585\n",
      "Epoch [16/25], step [58880/91700], Train Loss: 0.5032, Valid Loss: 0.5580\n",
      "Epoch [17/25], step [62560/91700], Train Loss: 0.5060, Valid Loss: 0.5540\n",
      "Epoch [18/25], step [66240/91700], Train Loss: 0.5030, Valid Loss: 0.5529\n",
      "Epoch [19/25], step [69920/91700], Train Loss: 0.4992, Valid Loss: 0.5524\n",
      "Epoch [20/25], step [73600/91700], Train Loss: 0.5046, Valid Loss: 0.5507\n",
      "Epoch [21/25], step [77280/91700], Train Loss: 0.5023, Valid Loss: 0.5498\n",
      "Epoch [22/25], step [80960/91700], Train Loss: 0.4979, Valid Loss: 0.5484\n",
      "Epoch [23/25], step [84640/91700], Train Loss: 0.5022, Valid Loss: 0.5481\n",
      "Epoch [24/25], step [88320/91700], Train Loss: 0.5035, Valid Loss: 0.5467\n",
      "Epoch [25/25], step [92000/91700], Train Loss: 0.4967, Valid Loss: 0.5457\n",
      "[2021-06-11 14:22:25.867702] -- Training new layers ended\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{datetime.now()}] -- Training new layers started\")\n",
    "train(model=model, train_data_loader=train_data_loader, val_data_loader=val_data_loader, optimizer=optimizer, \n",
    "      results = results, scheduler=scheduler, num_epochs=NUM_EPOCHS_1, train_whole_model = False)\n",
    "print(f\"[{datetime.now()}] -- Training new layers ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-parent",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Clear GPU cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "civil-clause",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-merchant",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train the whole model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-section",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Define optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "mature-concentration",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr2)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=steps_per_epoch*2,  num_training_steps=steps_per_epoch*NUM_EPOCHS_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "determined-british",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-11 14:22:25.908996] -- Training whole layers started\n",
      "Epoch [1/50], step [3680/183400], Train Loss: 0.4982, Valid Loss: 0.5445\n",
      "Epoch [2/50], step [7360/183400], Train Loss: 0.4943, Valid Loss: 0.5413\n",
      "Epoch [3/50], step [11040/183400], Train Loss: 0.4838, Valid Loss: 0.5358\n",
      "Epoch [4/50], step [14720/183400], Train Loss: 0.4800, Valid Loss: 0.5284\n",
      "Epoch [5/50], step [18400/183400], Train Loss: 0.4590, Valid Loss: 0.5197\n",
      "Epoch [6/50], step [22080/183400], Train Loss: 0.4449, Valid Loss: 0.5098\n",
      "Epoch [7/50], step [25760/183400], Train Loss: 0.4257, Valid Loss: 0.4988\n",
      "Epoch [8/50], step [29440/183400], Train Loss: 0.4061, Valid Loss: 0.4877\n",
      "Epoch [9/50], step [33120/183400], Train Loss: 0.3840, Valid Loss: 0.4770\n",
      "Epoch [10/50], step [36800/183400], Train Loss: 0.3676, Valid Loss: 0.4669\n",
      "Epoch [11/50], step [40480/183400], Train Loss: 0.3576, Valid Loss: 0.4589\n",
      "Epoch [12/50], step [44160/183400], Train Loss: 0.3325, Valid Loss: 0.4526\n",
      "Epoch [13/50], step [47840/183400], Train Loss: 0.3258, Valid Loss: 0.4480\n",
      "Epoch [14/50], step [51520/183400], Train Loss: 0.3013, Valid Loss: 0.4434\n",
      "Epoch [15/50], step [55200/183400], Train Loss: 0.2951, Valid Loss: 0.4399\n",
      "Epoch [16/50], step [58880/183400], Train Loss: 0.2850, Valid Loss: 0.4375\n",
      "Epoch [17/50], step [62560/183400], Train Loss: 0.2707, Valid Loss: 0.4343\n",
      "Epoch [18/50], step [66240/183400], Train Loss: 0.2569, Valid Loss: 0.4324\n",
      "Epoch [19/50], step [69920/183400], Train Loss: 0.2408, Valid Loss: 0.4313\n",
      "Epoch [20/50], step [73600/183400], Train Loss: 0.2312, Valid Loss: 0.4289\n",
      "Epoch [21/50], step [77280/183400], Train Loss: 0.2074, Valid Loss: 0.4259\n",
      "Epoch [22/50], step [80960/183400], Train Loss: 0.2099, Valid Loss: 0.4260\n",
      "Epoch [23/50], step [84640/183400], Train Loss: 0.1953, Valid Loss: 0.4258\n",
      "Epoch [24/50], step [88320/183400], Train Loss: 0.1727, Valid Loss: 0.4233\n",
      "Epoch [25/50], step [92000/183400], Train Loss: 0.1470, Valid Loss: 0.4233\n",
      "Epoch [26/50], step [95680/183400], Train Loss: 0.1511, Valid Loss: 0.4252\n",
      "Epoch [27/50], step [99360/183400], Train Loss: 0.1310, Valid Loss: 0.4217\n",
      "Epoch [28/50], step [103040/183400], Train Loss: 0.1222, Valid Loss: 0.4215\n",
      "Epoch [29/50], step [106720/183400], Train Loss: 0.1143, Valid Loss: 0.4222\n",
      "Epoch [30/50], step [110400/183400], Train Loss: 0.0896, Valid Loss: 0.4213\n",
      "Epoch [31/50], step [114080/183400], Train Loss: 0.0999, Valid Loss: 0.4228\n",
      "Epoch [32/50], step [117760/183400], Train Loss: 0.0840, Valid Loss: 0.4191\n",
      "Epoch [33/50], step [121440/183400], Train Loss: 0.0814, Valid Loss: 0.4217\n",
      "Epoch [34/50], step [125120/183400], Train Loss: 0.0677, Valid Loss: 0.4197\n",
      "Epoch [35/50], step [128800/183400], Train Loss: 0.0589, Valid Loss: 0.4250\n",
      "Epoch [36/50], step [132480/183400], Train Loss: 0.0565, Valid Loss: 0.4211\n",
      "Epoch [37/50], step [136160/183400], Train Loss: 0.0479, Valid Loss: 0.4227\n",
      "Epoch [38/50], step [139840/183400], Train Loss: 0.0403, Valid Loss: 0.4217\n",
      "Epoch [39/50], step [143520/183400], Train Loss: 0.0372, Valid Loss: 0.4228\n",
      "Epoch [40/50], step [147200/183400], Train Loss: 0.0366, Valid Loss: 0.4192\n",
      "Epoch [41/50], step [150880/183400], Train Loss: 0.0388, Valid Loss: 0.4213\n",
      "Epoch [42/50], step [154560/183400], Train Loss: 0.0324, Valid Loss: 0.4217\n",
      "Epoch [43/50], step [158240/183400], Train Loss: 0.0318, Valid Loss: 0.4198\n",
      "Epoch [44/50], step [161920/183400], Train Loss: 0.0273, Valid Loss: 0.4228\n",
      "Epoch [45/50], step [165600/183400], Train Loss: 0.0342, Valid Loss: 0.4277\n",
      "Epoch [46/50], step [169280/183400], Train Loss: 0.0220, Valid Loss: 0.4229\n",
      "Epoch [47/50], step [172960/183400], Train Loss: 0.0224, Valid Loss: 0.4225\n",
      "Epoch [48/50], step [176640/183400], Train Loss: 0.0186, Valid Loss: 0.4224\n",
      "Epoch [49/50], step [180320/183400], Train Loss: 0.0222, Valid Loss: 0.4248\n",
      "Epoch [50/50], step [184000/183400], Train Loss: 0.0161, Valid Loss: 0.4250\n",
      "[2021-06-11 15:04:29.941764] -- Training whole layers ended\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{datetime.now()}] -- Training whole layers started\")\n",
    "train(model=model,  train_data_loader=train_data_loader,  val_data_loader=val_data_loader,  optimizer=optimizer, \n",
    "      results = results,  scheduler=scheduler,  num_epochs=NUM_EPOCHS_2, train_whole_model=True)\n",
    "print(f\"[{datetime.now()}] -- Training whole layers ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-keyboard",
   "metadata": {},
   "source": [
    "### Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "current-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_example(example, model, max_len):\n",
    "    feature_extractor = FeatureExtractor(tokenizer, max_len)\n",
    "    model.eval()\n",
    "    input_ids,input_mask,_ = feature_extractor.build_features(example)\n",
    "    input_ids = input_ids.reshape(1, -1)\n",
    "    input_mask = input_mask.reshape(1, -1)\n",
    "    result = model(input_ids=input_ids.to(device), attention_mask=input_mask.to(device))\n",
    "    result = result[0].detach().cpu()\n",
    "    return result[0].detach().cpu().numpy() , torch.argmax(result).numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "saved-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_print_example(example, model, max_seq_length):\n",
    "    _, result = test_single_example(example, model, max_seq_length)\n",
    "    print (f\"Sentence1: {example['sentence1']}\")\n",
    "    print (f\"Sentence2: {example['sentence2']}\")\n",
    "    print (f\"Predicted label = {result}\")\n",
    "    print (f\"  Correct label = {example['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-virus",
   "metadata": {},
   "source": [
    "### Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "approximate-retro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 0.9991821155943293\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in range(len(raw_train_data)):\n",
    "    example = raw_train_data[i]\n",
    "    prob, result  = test_single_example(example, model, max_seq_length)\n",
    "    if example['label'] == result:\n",
    "        cnt += 1\n",
    "\n",
    "print(f\"Train Accuracy = {cnt/len(raw_train_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-japanese",
   "metadata": {},
   "source": [
    "#### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sufficient-questionnaire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.8782608695652174\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in range(len( raw_test_data)):\n",
    "    example = raw_test_data[i]\n",
    "    _, result  = test_single_example(example, model, max_seq_length)\n",
    "    if example['label'] == result:\n",
    "        cnt += 1\n",
    "\n",
    "print(f\"Test Accuracy = {cnt/len(raw_test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-nickname",
   "metadata": {},
   "source": [
    "#### Testing different examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "floral-cleanup",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence1: Syria is a small country with great kitchen\n",
      "Sentence2: Syria is a beautiful country with delicious kitchen\n",
      "Predicted label = 1\n",
      "  Correct label = 1\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "    'sentence1': \"Syria is a small country with great kitchen\",\n",
    "    'sentence2': \"Syria is a beautiful country with delicious kitchen\",\n",
    "    'label' : 1\n",
    "}\n",
    "\n",
    "test_and_print_example(example, model, max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "norman-bulletin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence1: Syria is a small country with great kitchen\n",
      "Sentence2: ITMO is a descent university, with great professors\n",
      "Predicted label = 0\n",
      "  Correct label = 0\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "    'sentence1': \"Syria is a small country with great kitchen\",\n",
    "    'sentence2': 'ITMO is a descent university, with great professors',\n",
    "    'label' : 0\n",
    "}\n",
    "test_and_print_example(example, model, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "spanish-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence1: PCCW 's chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So .\n",
      "Sentence2: Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So .\n",
      "Predicted label = 1\n",
      "  Correct label = 1\n"
     ]
    }
   ],
   "source": [
    "example = raw_test_data[0]\n",
    "test_and_print_example(example, model, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "inclusive-manitoba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence1: The monkeys could track their progress by watching a schematic representation of the arm and its motions on a video screen .\n",
      "Sentence2: The arm was kept in a separate room , but the monkeys could track their progress by watching a representation of the arm and its motions on a video screen .\n",
      "Predicted label = 0\n",
      "  Correct label = 0\n"
     ]
    }
   ],
   "source": [
    "example = raw_test_data[425]\n",
    "test_and_print_example(example, model, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "specific-stations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence1: I have no doubt whatever that the evidence of Iraqi weapons of mass destruction will be there .\n",
      "Sentence2: \" I have said throughout ... I have absolutely no doubt about the existence of weapons of mass destruction .\n",
      "Predicted label = 1\n",
      "  Correct label = 1\n"
     ]
    }
   ],
   "source": [
    "example = raw_test_data[123]\n",
    "test_and_print_example(example, model, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-timing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
