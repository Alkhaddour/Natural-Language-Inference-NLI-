{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "natural language inference (NLI) task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2WHfx0qh9MaxX7PAtkkIm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YamenHabib/Natural-Language-Inference-NLI-/blob/main/natural_language_inference_(NLI)_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20SnPe7E17na",
        "outputId": "01fb6600-31eb-4659-e4a5-2a768cb74a83"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    COLAB = True\n",
        "    print(\"using Google CoLab\")\n",
        "except:\n",
        "    print(\"not using Google CoLab\")\n",
        "    COLAB = False"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using Google CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TVF8j6d1vwu",
        "outputId": "2e460691-2ace-4a31-ae00-754fa12e6d2c"
      },
      "source": [
        "if COLAB:\n",
        "    !pip install datasets\n",
        "    !pip install transformers==3\n",
        "    !pip install colorama"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.8)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.1.95)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3) (20.9)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3) (2.4.7)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (0.4.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYzGK6gX0uPd"
      },
      "source": [
        "import sys\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from torch.utils.data import RandomSampler, DataLoader, SequentialSampler, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
        "from colorama import Fore\n",
        "import pandas as pd"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9KC3M6O013v",
        "outputId": "a6233661-71bb-493d-f787-b90e79aa027c"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkMToIkt2Rtz",
        "outputId": "cb56d191-5cc1-4a6e-d67d-8d34a287836b"
      },
      "source": [
        "raw_train_data = load_dataset('glue', 'mrpc', split='train')\n",
        "raw_eval_data = load_dataset('glue', 'mrpc', split='validation')"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgklQSKMNdT0",
        "outputId": "c326c0b9-a848-44af-daff-42b4250e39b1"
      },
      "source": [
        "print(len(raw_train_data))\n",
        "raw_train_data[50]"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'idx': 58,\n",
              " 'label': 1,\n",
              " 'sentence1': 'Several of the questions asked by the audience in the fast-paced forum were new to the candidates .',\n",
              " 'sentence2': 'Several of the audience questions were new to the candidates as well .'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdr9pgg1L3ao"
      },
      "source": [
        "df = pd.DataFrame.from_records(raw_train_data)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "XFj1yGNvMCTH",
        "outputId": "1f2609a1-03e2-44df-8f1e-ee1825aa4e80"
      },
      "source": [
        "counts = df['label'].value_counts()\n",
        "print(f\"Count of label 0 (not_equivalent) is: {counts[0]} and for label 1 (equivalent) is: {counts[1]}\",\"\\n\\n\")\n",
        "counts.plot.bar()"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of label 0 (not_equivalent) is: 1194 and for label 1 (equivalent) is: 2474 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbf96e64d90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD1CAYAAAC87SVQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAM/0lEQVR4nO3dUYid9ZnH8e9vtd2LtdAUZ4NN4ka6KUu8WCtBhe5Fl7Ia7UXam6ILbZBCemGgQi827Y2lRXBha0FwhRRDI7QVoS0N21A3G7qUsrTNWMQaXdfB6poQNV3FdhG6qz57Mf+sp+lMZjKZnNE83w8c5pznfc+Z/4HhOyfvec8kVYUkqYc/WusFSJKmx+hLUiNGX5IaMfqS1IjRl6RGjL4kNXLxWi/gTC699NLavHnzWi9Dkt5RHnnkkV9X1cxC297W0d+8eTOzs7NrvQxJekdJ8txi2zy8I0mNGH1JamTJ6CfZlORHSZ5IcjTJ58b8S0mOJ3l0XG6auM8XkswleSrJDRPz7WM2l2TP+XlKkqTFLOeY/uvA56vqF0neAzyS5NDY9rWq+ofJnZNsBW4GrgTeD/xLkg+OzfcCfwMcA44kOVBVT6zGE5EkLW3J6FfVCeDEuP7bJE8CG85wlx3Ag1X1O+BXSeaAa8a2uap6BiDJg2Nfoy9JU3JWx/STbAY+BPxsjHYneSzJviTrxmwD8PzE3Y6N2WJzSdKULDv6SS4BvgPcXlW/Ae4DPgBcxfy/BL66GgtKsivJbJLZkydPrsZDSpKGZUU/ybuYD/43q+q7AFX1YlW9UVVvAl/nrUM4x4FNE3ffOGaLzX9PVe2tqm1VtW1mZsHPFkiSVmjJY/pJAtwPPFlVd0/MLxvH+wE+ATw+rh8AvpXkbubfyN0C/BwIsCXJFczH/mbgb1friaylzXt+sNZLuKA8e9fH1noJ0gVrOWfvfBj4FPDLJI+O2ReBW5JcBRTwLPBZgKo6muQh5t+gfR24rareAEiyG3gYuAjYV1VHV/G5SJKWsJyzd37C/Kv00x08w33uBO5cYH7wTPeTJJ1ffiJXkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGlky+kk2JflRkieSHE3yuTF/X5JDSZ4eX9eNeZLck2QuyWNJrp54rJ1j/6eT7Dx/T0uStJDlvNJ/Hfh8VW0FrgNuS7IV2AMcrqotwOFxG+BGYMu47ALug/lfEsAdwLXANcAdp35RSJKmY8noV9WJqvrFuP5b4ElgA7AD2D922w98fFzfATxQ834KvDfJZcANwKGqermqXgEOAdtX9dlIks7orI7pJ9kMfAj4GbC+qk6MTS8A68f1DcDzE3c7NmaLzSVJU7Ls6Ce5BPgOcHtV/WZyW1UVUKuxoCS7kswmmT158uRqPKQkaVhW9JO8i/ngf7OqvjvGL47DNoyvL435cWDTxN03jtli899TVXuraltVbZuZmTmb5yJJWsJyzt4JcD/wZFXdPbHpAHDqDJydwPcn5p8eZ/FcB7w6DgM9DFyfZN14A/f6MZMkTcnFy9jnw8CngF8meXTMvgjcBTyU5DPAc8Anx7aDwE3AHPAacCtAVb2c5CvAkbHfl6vq5VV5FpKkZVky+lX1EyCLbP7oAvsXcNsij7UP2Hc2C5QkrR4/kStJjRh9SWrE6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNLBn9JPuSvJTk8YnZl5IcT/LouNw0se0LSeaSPJXkhon59jGbS7Jn9Z+KJGkpy3ml/w1g+wLzr1XVVeNyECDJVuBm4Mpxn39MclGSi4B7gRuBrcAtY19J0hRdvNQOVfXjJJuX+Xg7gAer6nfAr5LMAdeMbXNV9QxAkgfHvk+c9YolSSt2Lsf0dyd5bBz+WTdmG4DnJ/Y5NmaLzSVJU7TS6N8HfAC4CjgBfHW1FpRkV5LZJLMnT55crYeVJLHC6FfVi1X1RlW9CXydtw7hHAc2Tey6ccwWmy/02HuraltVbZuZmVnJ8iRJi1hR9JNcNnHzE8CpM3sOADcn+eMkVwBbgJ8DR4AtSa5I8m7m3+w9sPJlS5JWYsk3cpN8G/gIcGmSY8AdwEeSXAUU8CzwWYCqOprkIebfoH0duK2q3hiPsxt4GLgI2FdVR1f92UiSzmg5Z+/cssD4/jPsfydw5wLzg8DBs1qdJGlV+YlcSWrE6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1MiS/zG6pHe2zXt+sNZLuGA8e9fH1noJ58xX+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpkSWjn2RfkpeSPD4xe1+SQ0meHl/XjXmS3JNkLsljSa6euM/Osf/TSXaen6cjSTqT5bzS/waw/bTZHuBwVW0BDo/bADcCW8ZlF3AfzP+SAO4ArgWuAe449YtCkjQ9S0a/qn4MvHzaeAewf1zfD3x8Yv5Azfsp8N4klwE3AIeq6uWqegU4xB/+IpEknWcrPaa/vqpOjOsvAOvH9Q3A8xP7HRuzxeaSpCk65zdyq6qAWoW1AJBkV5LZJLMnT55crYeVJLHy6L84Dtswvr405seBTRP7bRyzxeZ/oKr2VtW2qto2MzOzwuVJkhay0ugfAE6dgbMT+P7E/NPjLJ7rgFfHYaCHgeuTrBtv4F4/ZpKkKVry/8hN8m3gI8ClSY4xfxbOXcBDST4DPAd8cux+ELgJmANeA24FqKqXk3wFODL2+3JVnf7msCTpPFsy+lV1yyKbPrrAvgXctsjj7AP2ndXqJEmryk/kSlIjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiPnFP0kzyb5ZZJHk8yO2fuSHEry9Pi6bsyT5J4kc0keS3L1ajwBSdLyrcYr/b+uqquqatu4vQc4XFVbgMPjNsCNwJZx2QXctwrfW5J0Fs7H4Z0dwP5xfT/w8Yn5AzXvp8B7k1x2Hr6/JGkR5xr9Av45ySNJdo3Z+qo6Ma6/AKwf1zcAz0/c99iYSZKm5OJzvP9fVdXxJH8KHEry75Mbq6qS1Nk84PjlsQvg8ssvP8flSZImndMr/ao6Pr6+BHwPuAZ48dRhm/H1pbH7cWDTxN03jtnpj7m3qrZV1baZmZlzWZ4k6TQrjn6SP0nynlPXgeuBx4EDwM6x207g++P6AeDT4yye64BXJw4DSZKm4FwO76wHvpfk1ON8q6p+mOQI8FCSzwDPAZ8c+x8EbgLmgNeAW8/he0uSVmDF0a+qZ4C/XGD+X8BHF5gXcNtKv58k6dz5iVxJasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqxOhLUiNGX5IaMfqS1IjRl6RGjL4kNWL0JakRoy9JjRh9SWrE6EtSI0Zfkhox+pLUiNGXpEaMviQ1YvQlqRGjL0mNGH1JasToS1IjRl+SGjH6ktSI0ZekRoy+JDVi9CWpEaMvSY0YfUlqZOrRT7I9yVNJ5pLsmfb3l6TOphr9JBcB9wI3AluBW5JsneYaJKmzab/SvwaYq6pnqup/gAeBHVNegyS1dfGUv98G4PmJ28eAayd3SLIL2DVu/neSp6a0tg4uBX691otYSv5+rVegNfK2//l8B/1s/tliG6Yd/SVV1V5g71qv40KUZLaqtq31OqSF+PM5HdM+vHMc2DRxe+OYSZKmYNrRPwJsSXJFkncDNwMHprwGSWprqod3qur1JLuBh4GLgH1VdXSaa2jOw2Z6O/PncwpSVWu9BknSlPiJXElqxOhLUiNGX5Iaedudpy/pwpfkL5j/NP6GMToOHKiqJ9duVT34Sr+hJLeu9RrUV5K/Y/5PsAT4+bgE+LZ/hPH88+ydhpL8Z1VdvtbrUE9J/gO4sqr+97T5u4GjVbVlbVbWg4d3LlBJHltsE7B+mmuRTvMm8H7gudPml41tOo+M/oVrPXAD8Mpp8wD/Nv3lSP/vduBwkqd56w8wXg78ObB7zVbVhNG/cP0TcElVPXr6hiT/Ov3lSPOq6odJPsj8n1qffCP3SFW9sXYr68Fj+pLUiGfvSFIjRl+SGjH6ktSI0ZekRoy+JDXyfxdLFHTfWX/5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMPDee1f-OfW",
        "outputId": "3f5f39fc-3a07-4510-d83f-949bae55f999"
      },
      "source": [
        "# Initialize tokenizer.\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "len(tokenizer)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50265"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8o5sxcm-kW1"
      },
      "source": [
        "class glueDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, max_len):\n",
        "      self.max_len = max_len \n",
        "      self.dataset = dataset\n",
        "    \n",
        "    def _truncate_pair_of_tokens(self, tokens_a, tokens_b, ):\n",
        "      while True:\n",
        "          total_length = len(tokens_a) + len(tokens_b)\n",
        "          if total_length <= self.max_len - 3:\n",
        "              break\n",
        "          if len(tokens_a) > len(tokens_b):\n",
        "              tokens_a.pop()\n",
        "          else:\n",
        "              tokens_b.pop()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      example = self.dataset[idx]\n",
        "      tokens_a = tokenizer.tokenize(example[\"sentence1\"])\n",
        "      tokens_b = tokenizer.tokenize(example[\"sentence2\"])\n",
        "      self._truncate_pair_of_tokens(tokens_a, tokens_b)\n",
        "      tokens = []\n",
        "      #tokens.append(\"[CLS]\")\n",
        "      tokens.append(tokenizer.cls_token)\n",
        "      for token in tokens_a:\n",
        "          tokens.append(token)\n",
        "      #tokens.append(\"[SEP]\")\n",
        "      tokens.append(tokenizer.sep_token) \n",
        "      for token in tokens_b:\n",
        "          tokens.append(token)\n",
        "      #tokens.append(\"[SEP]\")\n",
        "      tokens.append(tokenizer.sep_token)\n",
        "      \n",
        "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "      input_mask = [1] * len(input_ids)\n",
        "      while len(input_ids) < self.max_len:\n",
        "          input_ids.append(0)\n",
        "          input_mask.append(0)\n",
        "      \n",
        "      input_ids   = torch.tensor(input_ids, dtype=torch.int64).to(device)\n",
        "      input_mask  = torch.tensor(input_mask, dtype=torch.float).to(device)\n",
        "      label       = torch.tensor(example[\"label\"], dtype=torch.int64).to(device)\n",
        "      return (input_ids, input_mask, label)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.dataset)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCpCHg8dITdM"
      },
      "source": [
        "batch_size = 16\n",
        "max_seq_length = 128"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzCa0ZxxBa17"
      },
      "source": [
        "train_dataset = glueDataset(raw_train_data, max_seq_length)\n",
        "train_data_loader = DataLoader(train_dataset, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umOL0a9dBfDM"
      },
      "source": [
        "eval_dataset = glueDataset(raw_eval_data, max_seq_length)\n",
        "eval_data_loader = DataLoader(eval_dataset, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wYnxp1I9paA"
      },
      "source": [
        "class ROBERTAClassifier(torch.nn.Module):\n",
        "    def __init__(self, dropout_rate=0.3):\n",
        "        super(ROBERTAClassifier, self).__init__()\n",
        "        \n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l1 = torch.nn.Linear(768, 128)\n",
        "        self.bn1 = torch.nn.LayerNorm(128)\n",
        "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l2 = torch.nn.Linear(128, 2) \n",
        "        torch.nn.init.xavier_uniform_(self.l1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.l2.weight)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        #input_ids=None, attention_mask=None, token_type_ids=None\n",
        "        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        x = self.d1(x)\n",
        "        x = self.l1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.nn.Tanh()(x)\n",
        "        x = self.d2(x)\n",
        "        x = self.l2(x)\n",
        "        return x"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxF8ix9tGMR_"
      },
      "source": [
        "#object to hold out our results and to save and reload model and metrics file\n",
        "class ResultsSaver():\n",
        "  def __init__(self, train_len, val_len,output_path):\n",
        "    self.train_losses = []\n",
        "    self.val_losses = []\n",
        "    self.steps = []\n",
        "\n",
        "    self.best_val_loss = float('Inf')\n",
        "    \n",
        "    self.train_len = train_len\n",
        "    self.val_len = val_len\n",
        "    \n",
        "    self.output_path = output_path\n",
        "  \n",
        "  def save_checkpoint(self, path, model, valid_loss):\n",
        "    torch.save({'model_state_dict': model.state_dict(),'valid_loss': valid_loss}, self.output_path + path)\n",
        "\n",
        "  def load_checkpoint(self, path, model):    \n",
        "    state_dict = torch.load(self.output_path + path, map_location=device)\n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "  def save_metrics(self, path):   \n",
        "    state_dict = {'train_losses': self.train_losses,\n",
        "                  'val_losses': self.val_losses,\n",
        "                  'steps': self.steps}\n",
        "    \n",
        "    torch.save(state_dict, self.output_path + path)\n",
        "  \n",
        "  def load_metrics(self, path):    \n",
        "    state_dict = torch.load(self.output_path + path, map_location=device)\n",
        "    return state_dict['train_losses'], state_dict['val_losses'], state_dict['steps']\n",
        "\n",
        "  def update_train_val_loss(self, model, train_loss, val_loss, step, epoch, num_epochs):\n",
        "\n",
        "    train_loss = train_loss / self.train_len\n",
        "    val_loss = val_loss / self.val_len\n",
        "    self.train_losses.append(train_loss)\n",
        "    self.val_losses.append(val_loss)\n",
        "    self.steps.append(step)\n",
        "    print('Epoch [{}/{}], step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}' .format(epoch+1, num_epochs, step, num_epochs * self.train_len, train_loss, val_loss))\n",
        "    \n",
        "    # checkpoint\n",
        "    if self.best_val_loss > val_loss:\n",
        "        self.best_val_loss = val_loss\n",
        "        self.save_checkpoint('/model.pkl', model, self.best_val_loss)\n",
        "        self.save_metrics('/metric.pkl')"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzWh4blm-xRT"
      },
      "source": [
        "# defin training procedure\n",
        "def train(model, optimizer, train_data_loader, eval_data_loader, results, scheduler = None, num_epochs = 5 , train_whole_model = False):\n",
        "    step = 0\n",
        "    # if we want to train all the model (our added layer + roBERTa)\n",
        "    if train_whole_model:\n",
        "      for param in model.roberta.parameters():\n",
        "        param.requires_grad = True\n",
        "    # in case we just want to train our added layer.\n",
        "    else:\n",
        "      for param in model.roberta.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0.0                \n",
        "        val_loss = 0.0\n",
        "        for (input_ids, input_mask, y_true) in train_data_loader:\n",
        "            y_pred = model(input_ids = input_ids, attention_mask = input_mask) \n",
        "            loss = torch.nn.CrossEntropyLoss()(y_pred, y_true)\n",
        "            loss.backward()\n",
        "            # Optimizer and scheduler step\n",
        "            optimizer.step()    \n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            # Update train loss and step\n",
        "            train_loss += loss.item()\n",
        "            step += batch_size\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():                    \n",
        "            for (input_ids, input_mask, y_true) in eval_data_loader:\n",
        "                y_pred = model(input_ids = input_ids, attention_mask = input_mask)\n",
        "                loss = torch.nn.CrossEntropyLoss()(y_pred, y_true)\n",
        "                val_loss += loss.item()\n",
        "        results.update_train_val_loss(model, train_loss, val_loss, step, epoch, num_epochs)       \n",
        "        model.train()\n",
        "\n",
        "    results.save_metrics('/metric.pkl')"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QszkHFJnG5Gt"
      },
      "source": [
        "output_path = 'here'"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCP16EayI7cb"
      },
      "source": [
        "steps_per_epoch = len(train_dataset)\n",
        "model = ROBERTAClassifier(0.4)\n",
        "model = model.to(device)\n",
        "# Main training loop\n",
        "results = ResultsSaver(len(train_dataset), len(eval_dataset), output_path)"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbYrGDhP-xPg",
        "outputId": "c2fd0f93-a88b-497e-c43e-61caa87eb972"
      },
      "source": [
        "NUM_EPOCHS_1 = 8\n",
        "print(\" ............. Training the added Layers only ............. \")\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,  num_warmup_steps=steps_per_epoch*1,  num_training_steps=steps_per_epoch*NUM_EPOCHS_1)\n",
        "\n",
        "train(model=model, train_data_loader=train_data_loader, eval_data_loader=eval_data_loader, optimizer=optimizer, \n",
        "      results = results, scheduler=scheduler, num_epochs=NUM_EPOCHS_1, train_whole_model = False)\n",
        "\n",
        "print(\" ............. Training the whole Model ............. \")\n",
        "NUM_EPOCHS_2 = 6\n",
        "optimizer = AdamW(model.parameters(), lr=2e-6)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=steps_per_epoch*2,  num_training_steps=steps_per_epoch*NUM_EPOCHS_2)\n",
        "\n",
        "train(model=model,  train_data_loader=train_data_loader,  eval_data_loader=eval_data_loader,  optimizer=optimizer, \n",
        "      results = results,  scheduler=scheduler,  num_epochs=NUM_EPOCHS_2, train_whole_model=True)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ............. Training the added Layers only ............. \n",
            "Epoch [1/8], step [3680/29344], Train Loss: 0.0446, Valid Loss: 0.0402\n",
            "Epoch [2/8], step [7360/29344], Train Loss: 0.0452, Valid Loss: 0.0403\n",
            "Epoch [3/8], step [11040/29344], Train Loss: 0.0443, Valid Loss: 0.0401\n",
            "Epoch [4/8], step [14720/29344], Train Loss: 0.0438, Valid Loss: 0.0398\n",
            "Epoch [5/8], step [18400/29344], Train Loss: 0.0426, Valid Loss: 0.0395\n",
            "Epoch [6/8], step [22080/29344], Train Loss: 0.0431, Valid Loss: 0.0398\n",
            "Epoch [7/8], step [25760/29344], Train Loss: 0.0420, Valid Loss: 0.0395\n",
            "Epoch [8/8], step [29440/29344], Train Loss: 0.0412, Valid Loss: 0.0395\n",
            " ............. Training the whole Model ............. \n",
            "Epoch [1/6], step [3680/22008], Train Loss: 0.0410, Valid Loss: 0.0395\n",
            "Epoch [2/6], step [7360/22008], Train Loss: 0.0412, Valid Loss: 0.0394\n",
            "Epoch [3/6], step [11040/22008], Train Loss: 0.0407, Valid Loss: 0.0393\n",
            "Epoch [4/6], step [14720/22008], Train Loss: 0.0404, Valid Loss: 0.0392\n",
            "Epoch [5/6], step [18400/22008], Train Loss: 0.0411, Valid Loss: 0.0389\n",
            "Epoch [6/6], step [22080/22008], Train Loss: 0.0400, Valid Loss: 0.0384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw7eng4GC4DN"
      },
      "source": [
        "def truncate_pair_of_tokens(tokens_a, tokens_b, max_len):\n",
        "      while True:\n",
        "          total_length = len(tokens_a) + len(tokens_b)\n",
        "          if total_length <= max_len - 3:\n",
        "              break\n",
        "          if len(tokens_a) > len(tokens_b):\n",
        "              tokens_a.pop()\n",
        "          else:\n",
        "              tokens_b.pop()"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrD1HwUfCp9u"
      },
      "source": [
        "def build_features(example, max_len = 128):\n",
        "  tokens_a = tokenizer.tokenize(example[\"sentence1\"])\n",
        "  tokens_b = tokenizer.tokenize(example[\"sentence2\"])\n",
        "  truncate_pair_of_tokens(tokens_a, tokens_b, max_len)\n",
        "  tokens = []\n",
        "  #tokens.append(\"[CLS]\")\n",
        "  tokens.append(tokenizer.cls_token)\n",
        "  for token in tokens_a:\n",
        "      tokens.append(token)\n",
        "  #tokens.append(\"[SEP]\")\n",
        "  tokens.append(tokenizer.sep_token) \n",
        "  for token in tokens_b:\n",
        "      tokens.append(token)\n",
        "  #tokens.append(\"[SEP]\")\n",
        "  tokens.append(tokenizer.sep_token)\n",
        "  \n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  input_mask = [1] * len(input_ids)\n",
        "  while len(input_ids) < max_len:\n",
        "      input_ids.append(0)\n",
        "      input_mask.append(0)\n",
        "  \n",
        "  input_ids   = torch.tensor(input_ids, dtype=torch.int64).to(device)\n",
        "  input_mask  = torch.tensor(input_mask, dtype=torch.float).to(device)\n",
        "  return (input_ids, input_mask)"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9ACAc_hG9_2",
        "outputId": "2f8c1996-f358-4829-d3f2-229e4b6b4ec3"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ROBERTAClassifier(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (d1): Dropout(p=0.4, inplace=False)\n",
              "  (l1): Linear(in_features=768, out_features=128, bias=True)\n",
              "  (bn1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (d2): Dropout(p=0.4, inplace=False)\n",
              "  (l2): Linear(in_features=128, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en8546Bk5Be0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09ad90b-d0e6-4aa0-bda3-144e2677deba"
      },
      "source": [
        "example = {\n",
        "        'sentence1': 'The rain in Spain falls mainly on the plain.',\n",
        "        'sentence2': 'The rain in Spain falls mainly on the plain.',\n",
        "    }\n",
        "\n",
        "input_word_ids_test, input_masks_test = build_features(example)\n",
        "input_word_ids_test = input_word_ids_test.reshape(1, -1)\n",
        "input_masks_test = input_masks_test.reshape(1, -1)\n",
        "result = model(input_ids=input_word_ids_test.to(device),\n",
        "               attention_mask=input_masks_test.to(device))\n",
        "result = result[0].detach().cpu()\n",
        "print(result.numpy())\n",
        "result = torch.argmax(result).numpy()\n",
        "print(result)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.59095186  0.29047757]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dXiPQBkDwgX",
        "outputId": "40cc3878-4f3b-4329-9884-487100c0eca5"
      },
      "source": [
        "example = {\n",
        "        'sentence1': 'The rain in Spain falls mainly on the plain.',\n",
        "        'sentence2': 'Syria is a great country',\n",
        "    }\n",
        "\n",
        "input_word_ids_test, input_masks_test = build_features(example)\n",
        "input_word_ids_test = input_word_ids_test.reshape(1, -1)\n",
        "input_masks_test = input_masks_test.reshape(1, -1)\n",
        "result = model(input_ids=input_word_ids_test.to(device), attention_mask=input_masks_test.to(device))\n",
        "result = result[0].detach().cpu()\n",
        "print(result.numpy())\n",
        "result = torch.argmax(result).numpy()\n",
        "print(result)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.5440463   0.25686145]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLueevtuUqCf",
        "outputId": "de2c1da8-610c-41a8-b079-b5a4975c6773"
      },
      "source": [
        "raw_train_data[500]"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'idx': 559,\n",
              " 'label': 0,\n",
              " 'sentence1': 'The conservancy helped in the purchase of the land from the Estate of Samuel Mills Damon for $ 22 million .',\n",
              " 'sentence2': 'The park service purchased the land from the estate of Samuel Mills Damon for $ 22 million .'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpRh2J8lU2fp",
        "outputId": "ef6e5e6a-aebf-4140-cedb-daaca383aea2"
      },
      "source": [
        "example = {\n",
        "    'sentence1': 'The conservancy helped in the purchase of the land from the Estate of Samuel Mills Damon for $ 22 million .',\n",
        "    'sentence2': 'The park service purchased the land from the estate of Samuel Mills Damon for $ 22 million .'}\n",
        "\n",
        "input_word_ids_test, input_masks_test = build_features(example)\n",
        "input_word_ids_test = input_word_ids_test.reshape(1, -1)\n",
        "input_masks_test = input_masks_test.reshape(1, -1)\n",
        "result = model(input_ids=input_word_ids_test.to(device), attention_mask=input_masks_test.to(device))\n",
        "result = result[0].detach().cpu()\n",
        "print(result.numpy())\n",
        "result = torch.argmax(result).numpy()\n",
        "print(result)"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.72381943  0.41474888]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb5vuun7VvXd",
        "outputId": "ffa8e387-ce20-4626-add6-6bbd9431eb7a"
      },
      "source": [
        "raw_test_data = load_dataset('glue', 'mrpc', split='test')\n",
        "len(raw_test_data)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1725"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXy5sC2VZSyT",
        "outputId": "d80cd249-e83d-49d6-d75b-412234e2209f"
      },
      "source": [
        "raw_test_data[50]"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'idx': 50,\n",
              " 'label': 1,\n",
              " 'sentence1': 'The procedure is generally performed in the second or third trimester .',\n",
              " 'sentence2': 'The technique is used during the second and , occasionally , third trimester of pregnancy .'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs0wFHL1Ytc1"
      },
      "source": [
        "test_dataset = glueDataset(raw_test_data, max_seq_length)\n",
        "test_data_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l75xC9YGZNHE"
      },
      "source": [
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():                    \n",
        "    for (input_ids, input_mask, y_true) in test_data_loader:\n",
        "        y_pred = model(input_ids = input_ids, attention_mask = input_mask)\n",
        "        loss = torch.nn.CrossEntropyLoss()(y_pred, y_true)\n",
        "        test_loss += loss.item()\n",
        "test_loss /= len(raw_test_data)"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhLHAohBZf0i",
        "outputId": "dc91e44b-055e-4525-aadf-34875aeabcdc"
      },
      "source": [
        "test_loss"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03873657544453939"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GVgal5RZkZR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}