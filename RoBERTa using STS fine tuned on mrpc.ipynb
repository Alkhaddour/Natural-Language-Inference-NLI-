{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enormous-seeking",
   "metadata": {},
   "source": [
    "#### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cathedral-drama",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import RandomSampler, DataLoader, SequentialSampler, TensorDataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "from colorama import Fore\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-lighter",
   "metadata": {},
   "source": [
    "#### Set accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "czech-holly",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-sunset",
   "metadata": {},
   "source": [
    "#### Set data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brutal-expansion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = './models'\n",
    "pretrained_STS= 'pretrained_STS.pkl'\n",
    "fineTunedModel = 'model.pkl'\n",
    "FROM_FILE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "posted-farming",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-escape",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dying-arctic",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16           \n",
    "max_seq_length = 128\n",
    "lr1 = 1e-3             # learning rate while training the additional layers\n",
    "lr2 = 1e-6             # learning rate while training the whole model\n",
    "NUM_EPOCHS_1 = 20      # Number of epochs used to train new layers\n",
    "NUM_EPOCHS_2 = 40      # Number of epochs used to train the whole new layers\n",
    "dropout_rate = 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-celebrity",
   "metadata": {},
   "source": [
    "#### Set tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "musical-sperm",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-cherry",
   "metadata": {},
   "source": [
    "#### Define dataset reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "blind-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "class glueDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len):\n",
    "        self.max_len = max_len \n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def _truncate_pair_of_tokens(self, tokens_a, tokens_b, ):\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= self.max_len - 3:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        tokens_a = tokenizer.tokenize(example[\"sentence1\"])\n",
    "        tokens_b = tokenizer.tokenize(example[\"sentence2\"])\n",
    "        self._truncate_pair_of_tokens(tokens_a, tokens_b)\n",
    "        tokens = []\n",
    "        #tokens.append(\"[CLS]\")\n",
    "        tokens.append(tokenizer.cls_token)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "        #tokens.append(\"[SEP]\")\n",
    "        tokens.append(tokenizer.sep_token) \n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "        #tokens.append(\"[SEP]\")\n",
    "        tokens.append(tokenizer.sep_token)\n",
    "      \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        while len(input_ids) < self.max_len:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "      \n",
    "        input_ids   = torch.tensor(input_ids, dtype=torch.int64).to(device)\n",
    "        input_mask  = torch.tensor(input_mask, dtype=torch.float).to(device)\n",
    "        label       = torch.tensor(example[\"label\"], dtype=torch.int64).to(device)\n",
    "        return (input_ids, input_mask, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-facing",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define datasets & Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "compressed-disease",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\alkha\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\alkha\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset glue (C:\\Users\\alkha\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "raw_train_data = load_dataset('glue', 'mrpc', split='train')\n",
    "raw_val_data   = load_dataset('glue', 'mrpc', split='validation')\n",
    "raw_test_data  = load_dataset('glue', 'mrpc', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "international-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = glueDataset(raw_train_data, max_seq_length)\n",
    "val_dataset   = glueDataset(raw_val_data,   max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "selective-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, shuffle=True,  batch_size=batch_size)\n",
    "val_data_loader   = DataLoader(val_dataset,   shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-coordination",
   "metadata": {},
   "source": [
    "#### Results manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "preceding-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsSaver():\n",
    "    def __init__(self, train_len, val_len, output_dir):\n",
    "        self.train_losses  = []\n",
    "        self.val_losses    = []\n",
    "        self.steps         = []\n",
    "        self.best_val_loss = float('Inf')\n",
    "        self.train_len     = train_len\n",
    "        self.val_len       = val_len\n",
    "        self.output_dir    = output_dir\n",
    "          \n",
    "    def save_checkpoint(self, filename, model, valid_loss):\n",
    "        torch.save({'model_state_dict': model.state_dict(),'valid_loss': valid_loss}, os.path.join(self.output_dir, filename))\n",
    "\n",
    "    def load_checkpoint(self, filename, model):    \n",
    "        state_dict = torch.load(os.path.join(self.output_dir , filename), map_location=device)\n",
    "        model.load_state_dict(state_dict['model_state_dict'])\n",
    "        return state_dict['valid_loss']\n",
    "\n",
    "    def save_metrics(self, filename):   \n",
    "        state_dict = {'train_losses': self.train_losses,\n",
    "                      'val_losses': self.val_losses,\n",
    "                      'steps': self.steps}\n",
    "\n",
    "        torch.save(state_dict, os.path.join(self.output_dir, filename))\n",
    "  \n",
    "    def load_metrics(self, filename):    \n",
    "        state_dict = torch.load(os.path.join(self.output_dir , filename), map_location=device)\n",
    "        return state_dict['train_losses'], state_dict['val_losses'], state_dict['steps']\n",
    "\n",
    "    def update_train_val_loss(self, model, train_loss, val_loss, step, epoch, num_epochs):\n",
    "        train_loss = train_loss \n",
    "        val_loss   = val_loss  \n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.steps.append(step)\n",
    "        print('Epoch [{}/{}], step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "              .format(epoch+1, num_epochs, step, num_epochs * self.train_len, train_loss, val_loss))\n",
    "    \n",
    "        # checkpoint\n",
    "        if self.best_val_loss > val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.save_checkpoint('FineTuned_model.pkl', model, self.best_val_loss)\n",
    "            self.save_metrics('FineTuned_metric.pkl')\n",
    "            \n",
    "results = ResultsSaver(len(train_dataset), len(val_dataset), output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-communist",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-joining",
   "metadata": {},
   "source": [
    "##### predefined part - Just remove last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "auburn-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROBERTAClassifier(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(ROBERTAClassifier, self).__init__()        \n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.d1      = torch.nn.Dropout(dropout_rate)\n",
    "        self.l1      = torch.nn.Linear(768, 128)\n",
    "        self.bn1     = torch.nn.LayerNorm(128)\n",
    "        self.d2      = torch.nn.Dropout(dropout_rate)\n",
    "        self.l2      = torch.nn.Linear(128, 1) \n",
    "        torch.nn.init.xavier_uniform_(self.l1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.l2.weight)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x    = self.d1(x)\n",
    "        x    = self.l1(x)\n",
    "        x    = self.bn1(x)\n",
    "        x    = torch.nn.Tanh()(x)\n",
    "        x    = self.d2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-mixture",
   "metadata": {},
   "source": [
    "##### Fine-Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "soviet-authorization",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ROBERTAClassifier_2(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(ROBERTAClassifier_2, self).__init__()   \n",
    "        self.part_model = ROBERTAClassifier(dropout_rate)\n",
    "        results.load_checkpoint(pretrained_STS, self.part_model)\n",
    "        self.l2 = torch.nn.Linear(128, 2)\n",
    "        self.l3 = torch.nn.Softmax(1)\n",
    "        torch.nn.init.xavier_uniform_(self.l2.weight)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x    = self.part_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x    = self.l2(x)    \n",
    "        x    = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-joshua",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Main train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "synthetic-evans",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model on cuda\n"
     ]
    }
   ],
   "source": [
    "model = ROBERTAClassifier_2(dropout_rate)\n",
    "model.to(device)\n",
    "print(\"model on\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "placed-zambia",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_data_loader, val_data_loader, results, \n",
    "          scheduler = None, num_epochs = 5 , train_whole_model = False):\n",
    "    step = 0\n",
    "    # if we want to train all the model (our added layers + RoBERTa)\n",
    "    if train_whole_model:\n",
    "        for param in model.part_model.parameters():\n",
    "            param.requires_grad = True\n",
    "    # in case we just want to train our added layer.\n",
    "    else:\n",
    "        for param in model.part_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss  = 0.0                \n",
    "        val_loss    = 0.0\n",
    "        batch_count = 0\n",
    "        for (input_ids, input_mask, y_true) in train_data_loader:\n",
    "            y_pred = model(input_ids = input_ids, attention_mask = input_mask) \n",
    "            loss = torch.nn.CrossEntropyLoss()(y_pred, y_true)\n",
    "            loss.backward()\n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            # Update train loss and step\n",
    "            train_loss += loss.item()\n",
    "            step += batch_size\n",
    "            batch_count+=1\n",
    "        train_loss /= batch_count\n",
    "        model.eval()\n",
    "        with torch.no_grad():      \n",
    "            batch_count = 0\n",
    "            for (input_ids, input_mask, y_true) in val_data_loader:\n",
    "                y_pred = model(input_ids = input_ids, attention_mask = input_mask) \n",
    "                loss = torch.nn.CrossEntropyLoss()(y_pred, y_true)\n",
    "                val_loss += loss.item()\n",
    "                batch_count+=1\n",
    "            val_loss /= batch_count\n",
    "        results.update_train_val_loss(model, train_loss, val_loss, step, epoch, num_epochs)       \n",
    "        model.train()\n",
    "\n",
    "    results.save_metrics('FineTuned_metric.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-surveillance",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create or load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "protective-semiconductor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = ResultsSaver(len(train_dataset), len(val_dataset), output_dir)\n",
    "steps_per_epoch = len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "proper-plasma",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FROM_FILE:\n",
    "    model = ROBERTAClassifier_2(dropout_rate)\n",
    "    results.load_checkpoint(fineTunedModel, model)\n",
    "else:    \n",
    "    model = ROBERTAClassifier_2(dropout_rate)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-integer",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train new layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-workstation",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Define optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "korean-recovery",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,  num_warmup_steps=steps_per_epoch * 1, num_training_steps = steps_per_epoch * NUM_EPOCHS_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "innovative-liechtenstein",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-10 09:06:36.623575] -- Training new layers started\n",
      "Epoch [1/20], step [3680/73360], Train Loss: 0.7447, Valid Loss: 0.7146\n",
      "Epoch [2/20], step [7360/73360], Train Loss: 0.6908, Valid Loss: 0.6461\n",
      "Epoch [3/20], step [11040/73360], Train Loss: 0.6448, Valid Loss: 0.6148\n",
      "Epoch [4/20], step [14720/73360], Train Loss: 0.6235, Valid Loss: 0.6091\n",
      "Epoch [5/20], step [18400/73360], Train Loss: 0.6194, Valid Loss: 0.6072\n",
      "Epoch [6/20], step [22080/73360], Train Loss: 0.6163, Valid Loss: 0.6051\n",
      "Epoch [7/20], step [25760/73360], Train Loss: 0.6144, Valid Loss: 0.6023\n",
      "Epoch [8/20], step [29440/73360], Train Loss: 0.6118, Valid Loss: 0.5977\n",
      "Epoch [9/20], step [33120/73360], Train Loss: 0.6076, Valid Loss: 0.5918\n",
      "Epoch [10/20], step [36800/73360], Train Loss: 0.6006, Valid Loss: 0.5855\n",
      "Epoch [11/20], step [40480/73360], Train Loss: 0.5965, Valid Loss: 0.5805\n",
      "Epoch [12/20], step [44160/73360], Train Loss: 0.5904, Valid Loss: 0.5744\n",
      "Epoch [13/20], step [47840/73360], Train Loss: 0.5876, Valid Loss: 0.5700\n",
      "Epoch [14/20], step [51520/73360], Train Loss: 0.5841, Valid Loss: 0.5655\n",
      "Epoch [15/20], step [55200/73360], Train Loss: 0.5784, Valid Loss: 0.5598\n",
      "Epoch [16/20], step [58880/73360], Train Loss: 0.5743, Valid Loss: 0.5584\n",
      "Epoch [17/20], step [62560/73360], Train Loss: 0.5754, Valid Loss: 0.5568\n",
      "Epoch [18/20], step [66240/73360], Train Loss: 0.5724, Valid Loss: 0.5503\n",
      "Epoch [19/20], step [69920/73360], Train Loss: 0.5690, Valid Loss: 0.5530\n",
      "Epoch [20/20], step [73600/73360], Train Loss: 0.5696, Valid Loss: 0.5449\n",
      "[2021-06-10 09:12:28.026984] -- Training new layers ended\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{datetime.now()}] -- Training new layers started\")\n",
    "train(model=model, train_data_loader=train_data_loader, val_data_loader=val_data_loader, optimizer=optimizer, \n",
    "      results = results, scheduler=scheduler, num_epochs=NUM_EPOCHS_1, train_whole_model = False)\n",
    "print(f\"[{datetime.now()}] -- Training new layers ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-century",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Clear GPU cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "empirical-dakota",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-tonight",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train the whole model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-chemistry",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Define optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cosmetic-street",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr2)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=steps_per_epoch*2,  num_training_steps=steps_per_epoch*NUM_EPOCHS_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "becoming-search",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-10 09:12:28.073447] -- Training whole layers started\n",
      "Epoch [1/40], step [3680/146720], Train Loss: 0.5645, Valid Loss: 0.5438\n",
      "Epoch [2/40], step [7360/146720], Train Loss: 0.5581, Valid Loss: 0.5400\n",
      "Epoch [3/40], step [11040/146720], Train Loss: 0.5548, Valid Loss: 0.5351\n",
      "Epoch [4/40], step [14720/146720], Train Loss: 0.5443, Valid Loss: 0.5267\n",
      "Epoch [5/40], step [18400/146720], Train Loss: 0.5363, Valid Loss: 0.5172\n",
      "Epoch [6/40], step [22080/146720], Train Loss: 0.5290, Valid Loss: 0.5060\n",
      "Epoch [7/40], step [25760/146720], Train Loss: 0.5165, Valid Loss: 0.4948\n",
      "Epoch [8/40], step [29440/146720], Train Loss: 0.4995, Valid Loss: 0.4785\n",
      "Epoch [9/40], step [33120/146720], Train Loss: 0.4902, Valid Loss: 0.4700\n",
      "Epoch [10/40], step [36800/146720], Train Loss: 0.4782, Valid Loss: 0.4545\n",
      "Epoch [11/40], step [40480/146720], Train Loss: 0.4644, Valid Loss: 0.4496\n",
      "Epoch [12/40], step [44160/146720], Train Loss: 0.4581, Valid Loss: 0.4436\n",
      "Epoch [13/40], step [47840/146720], Train Loss: 0.4489, Valid Loss: 0.4412\n",
      "Epoch [14/40], step [51520/146720], Train Loss: 0.4383, Valid Loss: 0.4409\n",
      "Epoch [15/40], step [55200/146720], Train Loss: 0.4343, Valid Loss: 0.4361\n",
      "Epoch [16/40], step [58880/146720], Train Loss: 0.4302, Valid Loss: 0.4360\n",
      "Epoch [17/40], step [62560/146720], Train Loss: 0.4220, Valid Loss: 0.4381\n",
      "Epoch [18/40], step [66240/146720], Train Loss: 0.4182, Valid Loss: 0.4361\n",
      "Epoch [19/40], step [69920/146720], Train Loss: 0.4129, Valid Loss: 0.4369\n",
      "Epoch [20/40], step [73600/146720], Train Loss: 0.4063, Valid Loss: 0.4376\n",
      "Epoch [21/40], step [77280/146720], Train Loss: 0.4034, Valid Loss: 0.4357\n",
      "Epoch [22/40], step [80960/146720], Train Loss: 0.3985, Valid Loss: 0.4316\n",
      "Epoch [23/40], step [84640/146720], Train Loss: 0.3958, Valid Loss: 0.4300\n",
      "Epoch [24/40], step [88320/146720], Train Loss: 0.3915, Valid Loss: 0.4309\n",
      "Epoch [25/40], step [92000/146720], Train Loss: 0.3861, Valid Loss: 0.4330\n",
      "Epoch [26/40], step [95680/146720], Train Loss: 0.3858, Valid Loss: 0.4322\n",
      "Epoch [27/40], step [99360/146720], Train Loss: 0.3800, Valid Loss: 0.4262\n",
      "Epoch [28/40], step [103040/146720], Train Loss: 0.3770, Valid Loss: 0.4264\n",
      "Epoch [29/40], step [106720/146720], Train Loss: 0.3736, Valid Loss: 0.4361\n",
      "Epoch [30/40], step [110400/146720], Train Loss: 0.3737, Valid Loss: 0.4318\n",
      "Epoch [31/40], step [114080/146720], Train Loss: 0.3678, Valid Loss: 0.4301\n",
      "Epoch [32/40], step [117760/146720], Train Loss: 0.3683, Valid Loss: 0.4291\n",
      "Epoch [33/40], step [121440/146720], Train Loss: 0.3668, Valid Loss: 0.4326\n",
      "Epoch [34/40], step [125120/146720], Train Loss: 0.3662, Valid Loss: 0.4270\n",
      "Epoch [35/40], step [128800/146720], Train Loss: 0.3626, Valid Loss: 0.4205\n",
      "Epoch [36/40], step [132480/146720], Train Loss: 0.3612, Valid Loss: 0.4200\n",
      "Epoch [37/40], step [136160/146720], Train Loss: 0.3593, Valid Loss: 0.4273\n",
      "Epoch [38/40], step [139840/146720], Train Loss: 0.3580, Valid Loss: 0.4362\n",
      "Epoch [39/40], step [143520/146720], Train Loss: 0.3575, Valid Loss: 0.4286\n",
      "Epoch [40/40], step [147200/146720], Train Loss: 0.3533, Valid Loss: 0.4289\n",
      "[2021-06-10 09:46:31.261847] -- Training whole layers ended\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{datetime.now()}] -- Training whole layers started\")\n",
    "train(model=model,  train_data_loader=train_data_loader,  val_data_loader=val_data_loader,  optimizer=optimizer, \n",
    "      results = results,  scheduler=scheduler,  num_epochs=NUM_EPOCHS_2, train_whole_model=True)\n",
    "print(f\"[{datet ime.now()}] -- Training whole layers ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-claim",
   "metadata": {},
   "source": [
    "#### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adjustable-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_pair_of_tokens(tokens_a, tokens_b, max_len):\n",
    "      while True:\n",
    "          total_length = len(tokens_a) + len(tokens_b)\n",
    "          if total_length <= max_len - 3:\n",
    "              break\n",
    "          if len(tokens_a) > len(tokens_b):\n",
    "              tokens_a.pop()\n",
    "          else:\n",
    "              tokens_b.pop()\n",
    "\n",
    "def build_features(example, max_len = 128):\n",
    "  tokens_a = tokenizer.tokenize(example[\"sentence1\"])\n",
    "  tokens_b = tokenizer.tokenize(example[\"sentence2\"])\n",
    "  truncate_pair_of_tokens(tokens_a, tokens_b, max_len)\n",
    "  tokens = []\n",
    "  #tokens.append(\"[CLS]\")\n",
    "  tokens.append(tokenizer.cls_token)\n",
    "  for token in tokens_a:\n",
    "      tokens.append(token)\n",
    "  #tokens.append(\"[SEP]\")\n",
    "  tokens.append(tokenizer.sep_token) \n",
    "  for token in tokens_b:\n",
    "      tokens.append(token)\n",
    "  #tokens.append(\"[SEP]\")\n",
    "  tokens.append(tokenizer.sep_token)\n",
    "  \n",
    "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  input_mask = [1] * len(input_ids)\n",
    "  while len(input_ids) < max_len:\n",
    "      input_ids.append(0)\n",
    "      input_mask.append(0)\n",
    "  \n",
    "  input_ids   = torch.tensor(input_ids, dtype=torch.int64).to(device)\n",
    "  input_mask  = torch.tensor(input_mask, dtype=torch.float).to(device)\n",
    "  return (input_ids, input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "stunning-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "normal-wings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8753623188405797\n"
     ]
    }
   ],
   "source": [
    "final = 0\n",
    "for i in range(len(data)):\n",
    "  example = data[i]\n",
    "  input_word_ids_test, input_masks_test = build_features(example)\n",
    "  input_word_ids_test = input_word_ids_test.reshape(1, -1)\n",
    "  input_masks_test = input_masks_test.reshape(1, -1)\n",
    "  result = model(input_ids=input_word_ids_test.to(device), attention_mask=input_masks_test.to(device))\n",
    "  result = result[0].detach().cpu()\n",
    "  result = torch.argmax(result).numpy()\n",
    "  if example['label'] == result:\n",
    "    final += 1\n",
    "\n",
    "print(final/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-uruguay",
   "metadata": {},
   "source": [
    "#### Testing different examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "incorporated-motion",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.9208e-04, 9.9921e-01])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(1, dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = {\n",
    "      'sentence1': \"Syria is a small country with great kitchen\",\n",
    "      'sentence2': \"Syria is a beautiful country with delicious kitchen\",\n",
    "}\n",
    "input_word_ids_test, input_masks_test = build_features(example)\n",
    "input_word_ids_test = input_word_ids_test.reshape(1, -1)\n",
    "input_masks_test = input_masks_test.reshape(1, -1)\n",
    "result = model(input_ids=input_word_ids_test.to(device), attention_mask=input_masks_test.to(device))\n",
    "result = result[0].detach().cpu()\n",
    "print(result)\n",
    "result = torch.argmax(result).numpy()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "figured-washer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e+00, 1.9491e-07])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = {\n",
    "      'sentence1': \"Syria is a small country with great kitchen\",\n",
    "      'sentence2': 'Itmo is a descent university, with great it experts'\n",
    "}\n",
    "input_word_ids_test, input_masks_test = build_features(example)\n",
    "input_word_ids_test = input_word_ids_test.reshape(1, -1)\n",
    "input_masks_test = input_masks_test.reshape(1, -1)\n",
    "result = model(input_ids=input_word_ids_test.to(device), attention_mask=input_masks_test.to(device))\n",
    "result = result[0].detach().cpu()\n",
    "print(result)\n",
    "result = torch.argmax(result).numpy()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "iraqi-possession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCCW 's chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So . \n",
      " Current Chief Operating Officer Mike Butcher and Group Chief Financial Officer Alex Arena will report to So . \n",
      " 1\n",
      "tensor([1.8862e-09, 1.0000e+00])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(1, dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = raw_test_data[0]\n",
    "print(example['sentence1'],'\\n' ,example['sentence2'], '\\n', example['label'])\n",
    "input_word_ids_test, input_masks_test = build_features(example)\n",
    "input_word_ids_test = input_word_ids_test.reshape(1, -1)\n",
    "input_masks_test = input_masks_test.reshape(1, -1)\n",
    "result = model(input_ids=input_word_ids_test.to(device), attention_mask=input_masks_test.to(device))\n",
    "result = result[0].detach().cpu()\n",
    "print(result)\n",
    "result = torch.argmax(result).numpy()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "crude-black",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The monkeys could track their progress by watching a schematic representation of the arm and its motions on a video screen . \n",
      " The arm was kept in a separate room , but the monkeys could track their progress by watching a representation of the arm and its motions on a video screen . \n",
      " 0\n",
      "tensor([1.0000e+00, 1.6814e-08])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0, dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = raw_test_data[425]\n",
    "print(example['sentence1'],'\\n' ,example['sentence2'], '\\n', example['label'])\n",
    "input_word_ids_test, input_masks_test = build_features(example)\n",
    "input_word_ids_test = input_word_ids_test.reshape(1, -1)\n",
    "input_masks_test = input_masks_test.reshape(1, -1)\n",
    "result = model(input_ids=input_word_ids_test.to(device), attention_mask=input_masks_test.to(device))\n",
    "result = result[0].detach().cpu()\n",
    "print(result)\n",
    "result = torch.argmax(result).numpy()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-tunnel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
