{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "satisfactory-journalism",
   "metadata": {},
   "source": [
    "#### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comparative-evolution",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import RandomSampler, DataLoader, SequentialSampler, TensorDataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "from colorama import Fore\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-peninsula",
   "metadata": {},
   "source": [
    "#### Set accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "marked-enhancement",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-publication",
   "metadata": {},
   "source": [
    "#### Set data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "favorite-depth",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_src = \"./stsbenchmark/sts-train.csv\"\n",
    "val_src   = \"./stsbenchmark/sts-dev.csv\"\n",
    "test_src  = \"./stsbenchmark/sts-test.csv\"\n",
    "output_dir = './models'\n",
    "MODEL_FILE= 'model.pkl'\n",
    "FROM_FILE = False     # Load model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polish-state",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-charge",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "monthly-failure",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16           \n",
    "max_seq_length = 128\n",
    "lr1 = 1e-4             # learning rate while training the additional layers\n",
    "lr2 = 2e-6             # learning rate while training the whole model\n",
    "NUM_EPOCHS_1 = 25      # Number of epochs used to train new layers\n",
    "NUM_EPOCHS_2 = 50      # Number of epochs used to train the whole new layers\n",
    "dropout_rate = 0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-rouge",
   "metadata": {},
   "source": [
    "#### Set tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "settled-identity",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-investing",
   "metadata": {},
   "source": [
    "#### Define dataset reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "communist-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def truncate_pair_of_tokens(self, tokens_a, tokens_b):\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= self.max_len - 3:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "     \n",
    "    def build_features(self, example):\n",
    "        tokens_a = tokenizer.tokenize(example[\"sentence1\"])\n",
    "        tokens_b = tokenizer.tokenize(example[\"sentence2\"])\n",
    "        self.truncate_pair_of_tokens(tokens_a, tokens_b)\n",
    "        tokens = []\n",
    "        #tokens.append(\"[CLS]\")\n",
    "        tokens.append(tokenizer.cls_token)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "        #tokens.append(\"[SEP]\")\n",
    "        tokens.append(tokenizer.sep_token) \n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "        #tokens.append(\"[SEP]\")\n",
    "        tokens.append(tokenizer.sep_token)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        while len(input_ids) < self.max_len:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "\n",
    "        input_ids   = torch.tensor(input_ids, dtype=torch.int64).to(device)\n",
    "        input_mask  = torch.tensor(input_mask, dtype=torch.float).to(device)\n",
    "        return (input_ids, input_mask)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "certain-capacity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class STSBenchmark(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, max_len, tokenizer):\n",
    "        self.max_len = max_len \n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset = self.read_data()\n",
    "        self.feature_extractor =  FeatureExtractor(tokenizer, max_len)\n",
    "        \n",
    "    \n",
    "    def read_data(self):\n",
    "        raw_data = pd.read_csv(self.dataset_path ,sep =\"\\t\", names=list('1234567'), quoting=csv.QUOTE_NONE)\n",
    "        data = [{\"sentence1\":  raw_data['6'][i], \n",
    "                 \"sentence2\":  raw_data['7'][i], \n",
    "                 \"similarity\": raw_data['5'][i]/5} for i in range(len(raw_data)) ]\n",
    "        return data\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        input_ids,input_mask = self.feature_extractor.build_features(example)\n",
    "        similarity  = torch.tensor(example[\"similarity\"], dtype=torch.float32).to(device)\n",
    "        return (input_ids, input_mask, similarity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-material",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define datasets & Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "colonial-assault",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = STSBenchmark(train_src, max_seq_length, tokenizer)\n",
    "val_dataset   = STSBenchmark(val_src,   max_seq_length, tokenizer)\n",
    "test_dataset  = STSBenchmark(test_src,  max_seq_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prescribed-regular",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, shuffle=False, batch_size=batch_size)\n",
    "val_data_loader   = DataLoader(val_dataset,   shuffle=False, batch_size=batch_size)\n",
    "test_data_loader  = DataLoader(test_dataset,  shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-restriction",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "enormous-activity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ROBERTAClassifier(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(ROBERTAClassifier, self).__init__()        \n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.d1      = torch.nn.Dropout(dropout_rate)\n",
    "        self.l1      = torch.nn.Linear(768, 128)\n",
    "        self.bn1     = torch.nn.LayerNorm(128)\n",
    "        self.d2      = torch.nn.Dropout(dropout_rate)\n",
    "        self.l2      = torch.nn.Linear(128, 1) \n",
    "        torch.nn.init.xavier_uniform_(self.l1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.l2.weight)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x    = self.d1(x)\n",
    "        x    = self.l1(x)\n",
    "        x    = self.bn1(x)\n",
    "        x    = torch.nn.Tanh()(x)\n",
    "        x    = self.d2(x)\n",
    "        x    = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-camera",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Results manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "computational-lawrence",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResultsSaver():\n",
    "    def __init__(self, train_len, val_len, output_dir):\n",
    "        self.train_losses  = []\n",
    "        self.val_losses    = []\n",
    "        self.steps         = []\n",
    "        self.best_val_loss = float('Inf')\n",
    "        self.train_len     = train_len\n",
    "        self.val_len       = val_len\n",
    "        self.output_dir    = output_dir\n",
    "          \n",
    "    def save_checkpoint(self, filename, model, valid_loss):\n",
    "        torch.save({'model_state_dict': model.state_dict(),'valid_loss': valid_loss}, os.path.join(self.output_dir, filename))\n",
    "\n",
    "    def load_checkpoint(self, filename, model):    \n",
    "        state_dict = torch.load(os.path.join(self.output_dir , filename), map_location=device)\n",
    "        model.load_state_dict(state_dict['model_state_dict'])\n",
    "        return state_dict['valid_loss']\n",
    "\n",
    "    def save_metrics(self, filename):   \n",
    "        state_dict = {'train_losses': self.train_losses,\n",
    "                      'val_losses': self.val_losses,\n",
    "                      'steps': self.steps}\n",
    "\n",
    "        torch.save(state_dict, os.path.join(self.output_dir, filename))\n",
    "  \n",
    "    def load_metrics(self, filename):    \n",
    "        state_dict = torch.load(os.path.join(self.output_dir , filename), map_location=device)\n",
    "        return state_dict['train_losses'], state_dict['val_losses'], state_dict['steps']\n",
    "\n",
    "    def update_train_val_loss(self, model, train_loss, val_loss, step, epoch, num_epochs):\n",
    "        train_loss = train_loss \n",
    "        val_loss   = val_loss  \n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.steps.append(step)\n",
    "        print('Epoch [{}/{}], step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "              .format(epoch+1, num_epochs, step, num_epochs * self.train_len, train_loss, val_loss))\n",
    "    \n",
    "        # checkpoint\n",
    "        if self.best_val_loss > val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.save_checkpoint('model.pkl', model, self.best_val_loss)\n",
    "            self.save_metrics('metric.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-strain",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Main train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vertical-terminal",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_data_loader, val_data_loader, results, \n",
    "          scheduler = None, num_epochs = 5 , train_whole_model = False):\n",
    "    step = 0\n",
    "    # if we want to train all the model (our added layers + RoBERTa)\n",
    "    if train_whole_model:\n",
    "        for param in model.roberta.parameters():\n",
    "            param.requires_grad = True\n",
    "    # in case we just want to train our added layer.\n",
    "    else:\n",
    "        for param in model.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0                \n",
    "        val_loss = 0.0\n",
    "        batch_count = 0\n",
    "        for (input_ids, input_mask, sim_true) in train_data_loader:\n",
    "            sim_pred = model(input_ids = input_ids, attention_mask = input_mask)\n",
    "            loss = torch.nn.MSELoss()(sim_pred, sim_true.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            # Update train loss and step\n",
    "            train_loss += loss.item()\n",
    "            step += batch_size\n",
    "            batch_count+=1\n",
    "        train_loss /= batch_count\n",
    "        model.eval()\n",
    "        with torch.no_grad():      \n",
    "            batch_count = 0\n",
    "            for (input_ids, input_mask, sim_true) in val_data_loader:\n",
    "                sim_pred = model(input_ids = input_ids, attention_mask = input_mask)\n",
    "                loss = torch.nn.MSELoss()(sim_pred, sim_true.unsqueeze(1) )\n",
    "                val_loss += loss.item()\n",
    "                batch_count+=1\n",
    "            val_loss /= batch_count\n",
    "        results.update_train_val_loss(model, train_loss, val_loss, step, epoch, num_epochs)       \n",
    "        model.train()\n",
    "\n",
    "    results.save_metrics('metric.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-record",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create or load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "partial-professional",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = ResultsSaver(len(train_dataset), len(val_dataset), output_dir)\n",
    "steps_per_epoch = len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "terminal-heath",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FROM_FILE:\n",
    "    model = ROBERTAClassifier(dropout_rate)\n",
    "    results.load_checkpoint(MODEL_FILE, model)\n",
    "else:    \n",
    "    model = ROBERTAClassifier(dropout_rate)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-kernel",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train new layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-conversion",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Define optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "backed-completion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,  num_warmup_steps=steps_per_epoch * 1, num_training_steps = steps_per_epoch * NUM_EPOCHS_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "under-arbitration",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-05-10 13:31:59.642287] -- Training new layers started\n",
      "Epoch [1/25], step [5760/143725], Train Loss: 0.6151, Valid Loss: 0.1240\n",
      "Epoch [2/25], step [11520/143725], Train Loss: 0.3657, Valid Loss: 0.1237\n",
      "Epoch [3/25], step [17280/143725], Train Loss: 0.3147, Valid Loss: 0.1039\n",
      "Epoch [4/25], step [23040/143725], Train Loss: 0.2464, Valid Loss: 0.0986\n",
      "Epoch [5/25], step [28800/143725], Train Loss: 0.1958, Valid Loss: 0.1032\n",
      "Epoch [6/25], step [34560/143725], Train Loss: 0.1604, Valid Loss: 0.0949\n",
      "Epoch [7/25], step [40320/143725], Train Loss: 0.1349, Valid Loss: 0.0913\n",
      "Epoch [8/25], step [46080/143725], Train Loss: 0.1233, Valid Loss: 0.0930\n",
      "Epoch [9/25], step [51840/143725], Train Loss: 0.1133, Valid Loss: 0.0938\n",
      "Epoch [10/25], step [57600/143725], Train Loss: 0.1058, Valid Loss: 0.0946\n",
      "Epoch [11/25], step [63360/143725], Train Loss: 0.1027, Valid Loss: 0.0910\n",
      "Epoch [12/25], step [69120/143725], Train Loss: 0.0996, Valid Loss: 0.0907\n",
      "Epoch [13/25], step [74880/143725], Train Loss: 0.0960, Valid Loss: 0.0899\n",
      "Epoch [14/25], step [80640/143725], Train Loss: 0.0953, Valid Loss: 0.0886\n",
      "Epoch [15/25], step [86400/143725], Train Loss: 0.0940, Valid Loss: 0.0878\n",
      "Epoch [16/25], step [92160/143725], Train Loss: 0.0927, Valid Loss: 0.0878\n",
      "Epoch [17/25], step [97920/143725], Train Loss: 0.0910, Valid Loss: 0.0881\n",
      "Epoch [18/25], step [103680/143725], Train Loss: 0.0900, Valid Loss: 0.0878\n",
      "Epoch [19/25], step [109440/143725], Train Loss: 0.0881, Valid Loss: 0.0892\n",
      "Epoch [20/25], step [115200/143725], Train Loss: 0.0875, Valid Loss: 0.0898\n",
      "Epoch [21/25], step [120960/143725], Train Loss: 0.0867, Valid Loss: 0.0905\n",
      "Epoch [22/25], step [126720/143725], Train Loss: 0.0870, Valid Loss: 0.0905\n",
      "Epoch [23/25], step [132480/143725], Train Loss: 0.0862, Valid Loss: 0.0908\n",
      "Epoch [24/25], step [138240/143725], Train Loss: 0.0859, Valid Loss: 0.0907\n",
      "Epoch [25/25], step [144000/143725], Train Loss: 0.0855, Valid Loss: 0.0909\n",
      "[2021-05-10 13:44:31.621666] -- Training new layers ended\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{datetime.now()}] -- Training new layers started\")\n",
    "train(model=model, train_data_loader=train_data_loader, val_data_loader=val_data_loader, optimizer=optimizer, \n",
    "      results = results, scheduler=scheduler, num_epochs=NUM_EPOCHS_1, train_whole_model = False)\n",
    "print(f\"[{datetime.now()}] -- Training new layers ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-child",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Clear GPU cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "through-crossing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-wisconsin",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train the whole model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-article",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Define optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "international-boards",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr2)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=steps_per_epoch*2,  num_training_steps=steps_per_epoch*NUM_EPOCHS_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "divided-charge",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-05-10 13:44:31.645050] -- Training whole layers started\n",
      "Epoch [1/50], step [5760/287450], Train Loss: 0.0855, Valid Loss: 0.0914\n",
      "Epoch [2/50], step [11520/287450], Train Loss: 0.0843, Valid Loss: 0.0915\n",
      "Epoch [3/50], step [17280/287450], Train Loss: 0.0818, Valid Loss: 0.0883\n",
      "Epoch [4/50], step [23040/287450], Train Loss: 0.0719, Valid Loss: 0.0720\n",
      "Epoch [5/50], step [28800/287450], Train Loss: 0.0522, Valid Loss: 0.0444\n",
      "Epoch [6/50], step [34560/287450], Train Loss: 0.0398, Valid Loss: 0.0328\n",
      "Epoch [7/50], step [40320/287450], Train Loss: 0.0329, Valid Loss: 0.0286\n",
      "Epoch [8/50], step [46080/287450], Train Loss: 0.0286, Valid Loss: 0.0264\n",
      "Epoch [9/50], step [51840/287450], Train Loss: 0.0275, Valid Loss: 0.0250\n",
      "Epoch [10/50], step [57600/287450], Train Loss: 0.0253, Valid Loss: 0.0242\n",
      "Epoch [11/50], step [63360/287450], Train Loss: 0.0241, Valid Loss: 0.0234\n",
      "Epoch [12/50], step [69120/287450], Train Loss: 0.0226, Valid Loss: 0.0224\n",
      "Epoch [13/50], step [74880/287450], Train Loss: 0.0216, Valid Loss: 0.0225\n",
      "Epoch [14/50], step [80640/287450], Train Loss: 0.0209, Valid Loss: 0.0214\n",
      "Epoch [15/50], step [86400/287450], Train Loss: 0.0196, Valid Loss: 0.0212\n",
      "Epoch [16/50], step [92160/287450], Train Loss: 0.0186, Valid Loss: 0.0214\n",
      "Epoch [17/50], step [97920/287450], Train Loss: 0.0181, Valid Loss: 0.0226\n",
      "Epoch [18/50], step [103680/287450], Train Loss: 0.0177, Valid Loss: 0.0206\n",
      "Epoch [19/50], step [109440/287450], Train Loss: 0.0169, Valid Loss: 0.0199\n",
      "Epoch [20/50], step [115200/287450], Train Loss: 0.0158, Valid Loss: 0.0206\n",
      "Epoch [21/50], step [120960/287450], Train Loss: 0.0153, Valid Loss: 0.0193\n",
      "Epoch [22/50], step [126720/287450], Train Loss: 0.0151, Valid Loss: 0.0196\n",
      "Epoch [23/50], step [132480/287450], Train Loss: 0.0140, Valid Loss: 0.0201\n",
      "Epoch [24/50], step [138240/287450], Train Loss: 0.0136, Valid Loss: 0.0196\n",
      "Epoch [25/50], step [144000/287450], Train Loss: 0.0129, Valid Loss: 0.0190\n",
      "Epoch [26/50], step [149760/287450], Train Loss: 0.0129, Valid Loss: 0.0185\n",
      "Epoch [27/50], step [155520/287450], Train Loss: 0.0121, Valid Loss: 0.0199\n",
      "Epoch [28/50], step [161280/287450], Train Loss: 0.0116, Valid Loss: 0.0193\n",
      "Epoch [29/50], step [167040/287450], Train Loss: 0.0109, Valid Loss: 0.0190\n",
      "Epoch [30/50], step [172800/287450], Train Loss: 0.0107, Valid Loss: 0.0191\n",
      "Epoch [31/50], step [178560/287450], Train Loss: 0.0104, Valid Loss: 0.0193\n",
      "Epoch [32/50], step [184320/287450], Train Loss: 0.0096, Valid Loss: 0.0194\n",
      "Epoch [33/50], step [190080/287450], Train Loss: 0.0094, Valid Loss: 0.0183\n",
      "Epoch [34/50], step [195840/287450], Train Loss: 0.0090, Valid Loss: 0.0186\n",
      "Epoch [35/50], step [201600/287450], Train Loss: 0.0087, Valid Loss: 0.0180\n",
      "Epoch [36/50], step [207360/287450], Train Loss: 0.0089, Valid Loss: 0.0182\n",
      "Epoch [37/50], step [213120/287450], Train Loss: 0.0085, Valid Loss: 0.0185\n",
      "Epoch [38/50], step [218880/287450], Train Loss: 0.0080, Valid Loss: 0.0191\n",
      "Epoch [39/50], step [224640/287450], Train Loss: 0.0079, Valid Loss: 0.0178\n",
      "Epoch [40/50], step [230400/287450], Train Loss: 0.0076, Valid Loss: 0.0189\n",
      "Epoch [41/50], step [236160/287450], Train Loss: 0.0073, Valid Loss: 0.0210\n",
      "Epoch [42/50], step [241920/287450], Train Loss: 0.0072, Valid Loss: 0.0205\n",
      "Epoch [43/50], step [247680/287450], Train Loss: 0.0069, Valid Loss: 0.0193\n",
      "Epoch [44/50], step [253440/287450], Train Loss: 0.0070, Valid Loss: 0.0203\n",
      "Epoch [45/50], step [259200/287450], Train Loss: 0.0068, Valid Loss: 0.0204\n",
      "Epoch [46/50], step [264960/287450], Train Loss: 0.0065, Valid Loss: 0.0187\n",
      "Epoch [47/50], step [270720/287450], Train Loss: 0.0065, Valid Loss: 0.0196\n",
      "Epoch [48/50], step [276480/287450], Train Loss: 0.0062, Valid Loss: 0.0193\n",
      "Epoch [49/50], step [282240/287450], Train Loss: 0.0062, Valid Loss: 0.0226\n",
      "Epoch [50/50], step [288000/287450], Train Loss: 0.0059, Valid Loss: 0.0186\n",
      "[2021-05-10 14:54:19.674234] -- Training whole layers ended\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{datetime.now()}] -- Training whole layers started\")\n",
    "train(model=model,  train_data_loader=train_data_loader,  val_data_loader=val_data_loader,  optimizer=optimizer, \n",
    "      results = results,  scheduler=scheduler,  num_epochs=NUM_EPOCHS_2, train_whole_model=True)\n",
    "print(f\"[{datetime.now()}] -- Training whole layers ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-bankruptcy",
   "metadata": {},
   "source": [
    "#### Testing different examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "casual-summer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_single_example(example, model, max_len):\n",
    "    feature_extractor = FeatureExtractor(tokenizer, max_len)\n",
    "    model.eval()\n",
    "    input_ids,input_mask = feature_extractor.build_features(example)\n",
    "    input_ids = input_ids.reshape(1, -1)\n",
    "    input_mask = input_mask.reshape(1, -1)\n",
    "    result = model(input_ids=input_ids.to(device), attention_mask=input_mask.to(device))\n",
    "    result = result[0].detach().cpu()\n",
    "    return result.numpy()[0] * 5 #The model is measuring similarity on a scale from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "conventional-maldives",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.931507870554924\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "        'sentence1': 'It is raining in Syria',\n",
    "        'sentence2': 'Syria is a great country',\n",
    "    }\n",
    "\n",
    "res = test_single_example(example, model, max_seq_length)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "supposed-compact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2767534554004669\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "        'sentence1': 'It is sunny here',\n",
    "        'sentence2': 'UN chief condemns attack against peacekeepers in Mali',\n",
    "    }\n",
    "\n",
    "res = test_single_example(example, model, max_seq_length)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "proper-greensboro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.676664888858795\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "        'sentence1': \"A woman is slicing an onion.\",\n",
    "        'sentence2': 'A man is cutting an onion.',\n",
    "    }\n",
    "\n",
    "res = test_single_example(example, model, max_seq_length)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "interested-escape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2980884313583374\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "        'sentence1': \"I love rain\",\n",
    "        'sentence2': \"It is raining in SPb\",\n",
    "    }\n",
    "\n",
    "res = test_single_example(example, model, max_seq_length)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "distinct-formula",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.676341414451599\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "        'sentence1': \"A woman is slicing an onion.\",\n",
    "        'sentence2': 'A woman is cutting an onion.',\n",
    "    }\n",
    "\n",
    "res = test_single_example(example, model, max_seq_length)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "useful-brake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.878077208995819\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "        'sentence1': \"A woman is cutting an onion.\",\n",
    "        'sentence2': 'A woman is cutting an onion.',\n",
    "    }\n",
    "\n",
    "res = test_single_example(example, model, max_seq_length)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-christianity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
