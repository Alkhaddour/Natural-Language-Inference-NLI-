{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sophisticated-retail",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-poetry",
   "metadata": {},
   "source": [
    "In this notebook, we will train a RoBERTa-based model using [STS benchmark](http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-alpha",
   "metadata": {},
   "source": [
    "#### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "roman-screen",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import RandomSampler, DataLoader, SequentialSampler, TensorDataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "from colorama import Fore\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-divide",
   "metadata": {},
   "source": [
    "#### Set accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thirty-parliament",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-bottom",
   "metadata": {},
   "source": [
    "#### Set data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "consistent-british",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_src = \"./stsbenchmark/sts-train.csv\"\n",
    "val_src   = \"./stsbenchmark/sts-dev.csv\"\n",
    "test_src  = \"./stsbenchmark/sts-test.csv\"\n",
    "output_dir = './models'\n",
    "MODEL_FILE= 'model.pkl'\n",
    "FROM_FILE = False     # Load model from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "critical-problem",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-traffic",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wound-color",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16           \n",
    "max_seq_length = 128\n",
    "lr1 = 1e-4             # learning rate while training the additional layers\n",
    "lr2 = 2e-6             # learning rate while training the whole model\n",
    "NUM_EPOCHS_1 = 25      # Number of epochs used to train new layers\n",
    "NUM_EPOCHS_2 = 50      # Number of epochs used to train the whole new layers\n",
    "dropout_rate = 0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-message",
   "metadata": {},
   "source": [
    "#### Set tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "foreign-vegetarian",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-coaching",
   "metadata": {},
   "source": [
    "#### Define dataset reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "about-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def truncate_pair_of_tokens(self, tokens_a, tokens_b):\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= self.max_len - 3:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "     \n",
    "    def build_features(self, example):\n",
    "        tokens_a = tokenizer.tokenize(example[\"sentence1\"])\n",
    "        tokens_b = tokenizer.tokenize(example[\"sentence2\"])\n",
    "        self.truncate_pair_of_tokens(tokens_a, tokens_b)\n",
    "        tokens = []\n",
    "        #tokens.append(\"[CLS]\")\n",
    "        tokens.append(tokenizer.cls_token)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "        #tokens.append(\"[SEP]\")\n",
    "        tokens.append(tokenizer.sep_token) \n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "        #tokens.append(\"[SEP]\")\n",
    "        tokens.append(tokenizer.sep_token)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        while len(input_ids) < self.max_len:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "\n",
    "        input_ids   = torch.tensor(input_ids, dtype=torch.int64).to(device)\n",
    "        input_mask  = torch.tensor(input_mask, dtype=torch.float).to(device)\n",
    "        return (input_ids, input_mask)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "liked-twins",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class STSBenchmark(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, max_len, tokenizer):\n",
    "        self.max_len = max_len \n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset = self.read_data()\n",
    "        self.feature_extractor =  FeatureExtractor(tokenizer, max_len)\n",
    "        \n",
    "    \n",
    "    def read_data(self):\n",
    "        raw_data = pd.read_csv(self.dataset_path ,sep =\"\\t\", names=list('1234567'), quoting=csv.QUOTE_NONE)\n",
    "        data = [{\"sentence1\":  raw_data['6'][i], \n",
    "                 \"sentence2\":  raw_data['7'][i], \n",
    "                 \"similarity\": raw_data['5'][i]/5} for i in range(len(raw_data)) ]\n",
    "        return data\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        input_ids,input_mask = self.feature_extractor.build_features(example)\n",
    "        similarity  = torch.tensor(example[\"similarity\"], dtype=torch.float32).to(device)\n",
    "        return (input_ids, input_mask, similarity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-sacramento",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define datasets & Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "correct-aggregate",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = STSBenchmark(train_src, max_seq_length, tokenizer)\n",
    "val_dataset   = STSBenchmark(val_src,   max_seq_length, tokenizer)\n",
    "test_dataset  = STSBenchmark(test_src,  max_seq_length, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "brief-mainstream",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset, shuffle=False, batch_size=batch_size)\n",
    "val_data_loader   = DataLoader(val_dataset,   shuffle=False, batch_size=batch_size)\n",
    "test_data_loader  = DataLoader(test_dataset,  shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-lafayette",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-lightning",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ROBERTAClassifier(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(ROBERTAClassifier, self).__init__()        \n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.d1      = torch.nn.Dropout(dropout_rate)\n",
    "        self.l1      = torch.nn.Linear(768, 128)\n",
    "        self.bn1     = torch.nn.LayerNorm(128)\n",
    "        self.d2      = torch.nn.Dropout(dropout_rate)\n",
    "        self.l2      = torch.nn.Linear(128, 1) \n",
    "        torch.nn.init.xavier_uniform_(self.l1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.l2.weight)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x    = self.d1(x)\n",
    "        x    = self.l1(x)\n",
    "        x    = self.bn1(x)\n",
    "        x    = torch.nn.Tanh()(x)\n",
    "        x    = self.d2(x)\n",
    "        x    = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-spiritual",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Results manager\n",
    "Results manager is used to load and save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stunning-aberdeen",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResultsManager():\n",
    "    def __init__(self, train_len, val_len, output_dir):\n",
    "        self.train_losses  = []\n",
    "        self.val_losses    = []\n",
    "        self.steps         = []\n",
    "        self.best_val_loss = float('Inf')\n",
    "        self.train_len     = train_len\n",
    "        self.val_len       = val_len\n",
    "        self.output_dir    = output_dir\n",
    "          \n",
    "    def save_checkpoint(self, filename, model, valid_loss):\n",
    "        torch.save({'model_state_dict': model.state_dict(),'valid_loss': valid_loss}, os.path.join(self.output_dir, filename))\n",
    "\n",
    "    def load_checkpoint(self, filename, model):    \n",
    "        state_dict = torch.load(os.path.join(self.output_dir , filename), map_location=device)\n",
    "        model.load_state_dict(state_dict['model_state_dict'])\n",
    "        return state_dict['valid_loss']\n",
    "\n",
    "    def save_metrics(self, filename):   \n",
    "        state_dict = {'train_losses': self.train_losses,\n",
    "                      'val_losses': self.val_losses,\n",
    "                      'steps': self.steps}\n",
    "\n",
    "        torch.save(state_dict, os.path.join(self.output_dir, filename))\n",
    "  \n",
    "    def load_metrics(self, filename):    \n",
    "        state_dict = torch.load(os.path.join(self.output_dir , filename), map_location=device)\n",
    "        return state_dict['train_losses'], state_dict['val_losses'], state_dict['steps']\n",
    "\n",
    "    def update_train_val_loss(self, model, train_loss, val_loss, step, epoch, num_epochs):\n",
    "        train_loss = train_loss \n",
    "        val_loss   = val_loss  \n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.steps.append(step)\n",
    "        print('Epoch [{}/{}], step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "              .format(epoch+1, num_epochs, step, num_epochs * self.train_len, train_loss, val_loss))\n",
    "    \n",
    "        # checkpoint\n",
    "        if self.best_val_loss > val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.save_checkpoint('model.pkl', model, self.best_val_loss)\n",
    "            self.save_metrics('metric.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-burns",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Main train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "weekly-jerusalem",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_data_loader, val_data_loader, results, \n",
    "          scheduler = None, num_epochs = 5 , train_whole_model = False):\n",
    "    step = 0\n",
    "    # if we want to train all the model (our added layers + RoBERTa)\n",
    "    if train_whole_model:\n",
    "        for param in model.roberta.parameters():\n",
    "            param.requires_grad = True\n",
    "    # in case we just want to train our added layer.\n",
    "    else:\n",
    "        for param in model.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0                \n",
    "        val_loss = 0.0\n",
    "        batch_count = 0\n",
    "        for (input_ids, input_mask, sim_true) in train_data_loader:\n",
    "            sim_pred = model(input_ids = input_ids, attention_mask = input_mask)\n",
    "            loss = torch.nn.MSELoss()(sim_pred, sim_true.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            # Update train loss and step\n",
    "            train_loss += loss.item()\n",
    "            step += batch_size\n",
    "            batch_count+=1\n",
    "        train_loss /= batch_count\n",
    "        model.eval()\n",
    "        with torch.no_grad():      \n",
    "            batch_count = 0\n",
    "            for (input_ids, input_mask, sim_true) in val_data_loader:\n",
    "                sim_pred = model(input_ids = input_ids, attention_mask = input_mask)\n",
    "                loss = torch.nn.MSELoss()(sim_pred, sim_true.unsqueeze(1) )\n",
    "                val_loss += loss.item()\n",
    "                batch_count+=1\n",
    "            val_loss /= batch_count\n",
    "        results.update_train_val_loss(model, train_loss, val_loss, step, epoch, num_epochs)       \n",
    "        model.train()\n",
    "\n",
    "    results.save_metrics('metric.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-acquisition",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create or load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abstract-expense",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = ResultsManager(len(train_dataset), len(val_dataset), output_dir)\n",
    "steps_per_epoch = len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "surgical-quest",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FROM_FILE:\n",
    "    model = ROBERTAClassifier(dropout_rate)\n",
    "    results.load_checkpoint(MODEL_FILE, model)\n",
    "else:    \n",
    "    model = ROBERTAClassifier(dropout_rate)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-replication",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train new layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-asset",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Define optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "perceived-senior",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr1)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,  num_warmup_steps=steps_per_epoch * 1, num_training_steps = steps_per_epoch * NUM_EPOCHS_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "quarterly-venezuela",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-11 12:47:00.160565] -- Training new layers started\n",
      "Epoch [1/25], step [5760/143725], Train Loss: 0.5435, Valid Loss: 0.1203\n",
      "Epoch [2/25], step [11520/143725], Train Loss: 0.3644, Valid Loss: 0.1358\n",
      "Epoch [3/25], step [17280/143725], Train Loss: 0.3039, Valid Loss: 0.1063\n",
      "Epoch [4/25], step [23040/143725], Train Loss: 0.2411, Valid Loss: 0.1065\n",
      "Epoch [5/25], step [28800/143725], Train Loss: 0.1959, Valid Loss: 0.0984\n",
      "Epoch [6/25], step [34560/143725], Train Loss: 0.1575, Valid Loss: 0.0994\n",
      "Epoch [7/25], step [40320/143725], Train Loss: 0.1373, Valid Loss: 0.1012\n",
      "Epoch [8/25], step [46080/143725], Train Loss: 0.1202, Valid Loss: 0.0932\n",
      "Epoch [9/25], step [51840/143725], Train Loss: 0.1102, Valid Loss: 0.0923\n",
      "Epoch [10/25], step [57600/143725], Train Loss: 0.1056, Valid Loss: 0.0961\n",
      "Epoch [11/25], step [63360/143725], Train Loss: 0.1012, Valid Loss: 0.0953\n",
      "Epoch [12/25], step [69120/143725], Train Loss: 0.0988, Valid Loss: 0.0938\n",
      "Epoch [13/25], step [74880/143725], Train Loss: 0.0954, Valid Loss: 0.0896\n",
      "Epoch [14/25], step [80640/143725], Train Loss: 0.0934, Valid Loss: 0.0891\n",
      "Epoch [15/25], step [86400/143725], Train Loss: 0.0941, Valid Loss: 0.0902\n",
      "Epoch [16/25], step [92160/143725], Train Loss: 0.0913, Valid Loss: 0.0879\n",
      "Epoch [17/25], step [97920/143725], Train Loss: 0.0923, Valid Loss: 0.0906\n",
      "Epoch [18/25], step [103680/143725], Train Loss: 0.0883, Valid Loss: 0.0897\n",
      "Epoch [19/25], step [109440/143725], Train Loss: 0.0880, Valid Loss: 0.0909\n",
      "Epoch [20/25], step [115200/143725], Train Loss: 0.0873, Valid Loss: 0.0892\n",
      "Epoch [21/25], step [120960/143725], Train Loss: 0.0872, Valid Loss: 0.0922\n",
      "Epoch [22/25], step [126720/143725], Train Loss: 0.0858, Valid Loss: 0.0928\n",
      "Epoch [23/25], step [132480/143725], Train Loss: 0.0860, Valid Loss: 0.0911\n",
      "Epoch [24/25], step [138240/143725], Train Loss: 0.0855, Valid Loss: 0.0926\n",
      "Epoch [25/25], step [144000/143725], Train Loss: 0.0852, Valid Loss: 0.0935\n",
      "[2021-06-11 12:59:13.647225] -- Training new layers ended\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{datetime.now()}] -- Training new layers started\")\n",
    "train(model=model, train_data_loader=train_data_loader, val_data_loader=val_data_loader, optimizer=optimizer, \n",
    "      results = results, scheduler=scheduler, num_epochs=NUM_EPOCHS_1, train_whole_model = False)\n",
    "print(f\"[{datetime.now()}] -- Training new layers ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-transmission",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Clear GPU cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fundamental-deadline",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-northwest",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train the whole model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-compression",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Define optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hungarian-plane",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr2)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=steps_per_epoch*2,  num_training_steps=steps_per_epoch*NUM_EPOCHS_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "flexible-register",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-06-11 12:59:13.694904] -- Training whole layers started\n",
      "Epoch [1/50], step [5760/287450], Train Loss: 0.0848, Valid Loss: 0.0939\n",
      "Epoch [2/50], step [11520/287450], Train Loss: 0.0846, Valid Loss: 0.0935\n",
      "Epoch [3/50], step [17280/287450], Train Loss: 0.0819, Valid Loss: 0.0899\n",
      "Epoch [4/50], step [23040/287450], Train Loss: 0.0727, Valid Loss: 0.0734\n",
      "Epoch [5/50], step [28800/287450], Train Loss: 0.0514, Valid Loss: 0.0432\n",
      "Epoch [6/50], step [34560/287450], Train Loss: 0.0378, Valid Loss: 0.0327\n",
      "Epoch [7/50], step [40320/287450], Train Loss: 0.0317, Valid Loss: 0.0283\n",
      "Epoch [8/50], step [46080/287450], Train Loss: 0.0288, Valid Loss: 0.0264\n",
      "Epoch [9/50], step [51840/287450], Train Loss: 0.0265, Valid Loss: 0.0252\n",
      "Epoch [10/50], step [57600/287450], Train Loss: 0.0243, Valid Loss: 0.0244\n",
      "Epoch [11/50], step [63360/287450], Train Loss: 0.0239, Valid Loss: 0.0236\n",
      "Epoch [12/50], step [69120/287450], Train Loss: 0.0227, Valid Loss: 0.0230\n",
      "Epoch [13/50], step [74880/287450], Train Loss: 0.0212, Valid Loss: 0.0226\n",
      "Epoch [14/50], step [80640/287450], Train Loss: 0.0206, Valid Loss: 0.0216\n",
      "Epoch [15/50], step [86400/287450], Train Loss: 0.0195, Valid Loss: 0.0211\n",
      "Epoch [16/50], step [92160/287450], Train Loss: 0.0187, Valid Loss: 0.0213\n",
      "Epoch [17/50], step [97920/287450], Train Loss: 0.0175, Valid Loss: 0.0205\n",
      "Epoch [18/50], step [103680/287450], Train Loss: 0.0174, Valid Loss: 0.0203\n",
      "Epoch [19/50], step [109440/287450], Train Loss: 0.0165, Valid Loss: 0.0203\n",
      "Epoch [20/50], step [115200/287450], Train Loss: 0.0154, Valid Loss: 0.0204\n",
      "Epoch [21/50], step [120960/287450], Train Loss: 0.0150, Valid Loss: 0.0191\n",
      "Epoch [22/50], step [126720/287450], Train Loss: 0.0143, Valid Loss: 0.0195\n",
      "Epoch [23/50], step [132480/287450], Train Loss: 0.0137, Valid Loss: 0.0198\n",
      "Epoch [24/50], step [138240/287450], Train Loss: 0.0129, Valid Loss: 0.0199\n",
      "Epoch [25/50], step [144000/287450], Train Loss: 0.0128, Valid Loss: 0.0198\n",
      "Epoch [26/50], step [149760/287450], Train Loss: 0.0125, Valid Loss: 0.0190\n",
      "Epoch [27/50], step [155520/287450], Train Loss: 0.0119, Valid Loss: 0.0201\n",
      "Epoch [28/50], step [161280/287450], Train Loss: 0.0116, Valid Loss: 0.0186\n",
      "Epoch [29/50], step [167040/287450], Train Loss: 0.0109, Valid Loss: 0.0191\n",
      "Epoch [30/50], step [172800/287450], Train Loss: 0.0106, Valid Loss: 0.0201\n",
      "Epoch [31/50], step [178560/287450], Train Loss: 0.0099, Valid Loss: 0.0194\n",
      "Epoch [32/50], step [184320/287450], Train Loss: 0.0097, Valid Loss: 0.0190\n",
      "Epoch [33/50], step [190080/287450], Train Loss: 0.0094, Valid Loss: 0.0193\n",
      "Epoch [34/50], step [195840/287450], Train Loss: 0.0090, Valid Loss: 0.0189\n",
      "Epoch [35/50], step [201600/287450], Train Loss: 0.0089, Valid Loss: 0.0196\n",
      "Epoch [36/50], step [207360/287450], Train Loss: 0.0086, Valid Loss: 0.0189\n",
      "Epoch [37/50], step [213120/287450], Train Loss: 0.0081, Valid Loss: 0.0192\n",
      "Epoch [38/50], step [218880/287450], Train Loss: 0.0079, Valid Loss: 0.0183\n",
      "Epoch [39/50], step [224640/287450], Train Loss: 0.0080, Valid Loss: 0.0199\n",
      "Epoch [40/50], step [230400/287450], Train Loss: 0.0077, Valid Loss: 0.0198\n",
      "Epoch [41/50], step [236160/287450], Train Loss: 0.0070, Valid Loss: 0.0182\n",
      "Epoch [42/50], step [241920/287450], Train Loss: 0.0073, Valid Loss: 0.0192\n",
      "Epoch [43/50], step [247680/287450], Train Loss: 0.0071, Valid Loss: 0.0189\n",
      "Epoch [44/50], step [253440/287450], Train Loss: 0.0069, Valid Loss: 0.0214\n",
      "Epoch [45/50], step [259200/287450], Train Loss: 0.0070, Valid Loss: 0.0187\n",
      "Epoch [46/50], step [264960/287450], Train Loss: 0.0065, Valid Loss: 0.0186\n",
      "Epoch [47/50], step [270720/287450], Train Loss: 0.0063, Valid Loss: 0.0186\n",
      "Epoch [48/50], step [276480/287450], Train Loss: 0.0061, Valid Loss: 0.0182\n",
      "Epoch [49/50], step [282240/287450], Train Loss: 0.0061, Valid Loss: 0.0185\n",
      "Epoch [50/50], step [288000/287450], Train Loss: 0.0062, Valid Loss: 0.0191\n",
      "[2021-06-11 14:07:01.416439] -- Training whole layers ended\n"
     ]
    }
   ],
   "source": [
    "print(f\"[{datetime.now()}] -- Training whole layers started\")\n",
    "train(model=model,  train_data_loader=train_data_loader,  val_data_loader=val_data_loader,  optimizer=optimizer, \n",
    "      results = results,  scheduler=scheduler,  num_epochs=NUM_EPOCHS_2, train_whole_model=True)\n",
    "print(f\"[{datetime.now()}] -- Training whole layers ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-alcohol",
   "metadata": {},
   "source": [
    "#### Testing different examples\n",
    "The model should measure similarity on a scale from 0 (not similar) to 5 (very similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mounted-major",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_single_example(example, model, max_len):\n",
    "    feature_extractor = FeatureExtractor(tokenizer, max_len)\n",
    "    model.eval()\n",
    "    input_ids,input_mask = feature_extractor.build_features(example)\n",
    "    input_ids = input_ids.reshape(1, -1)\n",
    "    input_mask = input_mask.reshape(1, -1)\n",
    "    result = model(input_ids=input_ids.to(device), attention_mask=input_mask.to(device))\n",
    "    result = result[0].detach().cpu()\n",
    "    return result.numpy()[0] * 5 #The model is measuring similarity on a scale from 0 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "intellectual-integral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity 0.894031822681427\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "        'sentence1': 'It is raining in Syria',\n",
    "        'sentence2': 'Syria is a great country',\n",
    "    }\n",
    "\n",
    "sim = test_single_example(example, model, max_seq_length)\n",
    "print(f\"similarity {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "billion-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity 0.37564489990472794\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "        'sentence1': 'It is sunny here',\n",
    "        'sentence2': 'UN chief condemns attack against peacekeepers in Mali',\n",
    "    }\n",
    "\n",
    "sim = test_single_example(example, model, max_seq_length)\n",
    "print(f\"similarity {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mysterious-commons",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity 2.6991331577301025\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "        'sentence1': \"A woman is slicing an onion.\",\n",
    "        'sentence2': 'A man is cutting an onion.',\n",
    "    }\n",
    "\n",
    "sim = test_single_example(example, model, max_seq_length)\n",
    "print(f\"similarity {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "quick-aggregate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity 1.332910805940628\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "        'sentence1': \"I love rain\",\n",
    "        'sentence2': \"It is raining in SPb\",\n",
    "    }\n",
    "\n",
    "sim = test_single_example(example, model, max_seq_length)\n",
    "print(f\"similarity {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "working-visibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity 4.7999367117881775\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "        'sentence1': \"A woman is slicing an onion.\",\n",
    "        'sentence2': 'A woman is cutting an onion.',\n",
    "    }\n",
    "\n",
    "sim = test_single_example(example, model, max_seq_length)\n",
    "print(f\"similarity {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "stunning-acrylic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity 4.889133274555206\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "        'sentence1': \"A woman is cutting an onion.\",\n",
    "        'sentence2': 'A woman is cutting an onion.',\n",
    "    }\n",
    "\n",
    "sim = test_single_example(example, model, max_seq_length)\n",
    "print(f\"similarity {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-criticism",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
